{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32101906-67bb-42bf-9cf7-175d41d2fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19\n",
      "PyTorch: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.1\n",
      "GPU: NVIDIA RTX A6000\n",
      "GPU Memory: 48.31 GB\n",
      "\n",
      "‚úÖ Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected, using CPU\")\n",
    "    gpu_memory_gb = 0\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\n‚úÖ Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fe1ebb-2a9c-43ae-91b7-a04577281b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Vision models\n",
    "import timm\n",
    "try:\n",
    "    import open_clip\n",
    "    OPEN_CLIP_AVAILABLE = True\n",
    "except:\n",
    "    OPEN_CLIP_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è open_clip not available\")\n",
    "\n",
    "# Audio models\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "\n",
    "# NLP models\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except:\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è sentence-transformers not available\")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f4fbc0-22a3-4f85-a9be-98bc6b0b98ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using LARGE model configuration\n",
      "\n",
      "üìä Model Config:\n",
      "  - Preset: LARGE\n",
      "  - Model dim: 512\n",
      "  - Layers: 4\n",
      "  - Heads: 8\n",
      "  - Batch size: 8\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model architecture\"\"\"\n",
    "    \n",
    "    # Model size\n",
    "    preset: str = \"large\"\n",
    "    d_model: int = 512\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 4\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Encoders\n",
    "    vision_backbone: str = \"vit_base_patch16_224\"\n",
    "    audio_backbone: str = \"facebook/wav2vec2-large-960h\"\n",
    "    text_backbone: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    vision_pretrained: bool = True  # ‚Üê ADD THIS LINE\n",
    "    \n",
    "    freeze_vision: bool = True\n",
    "    freeze_audio: bool = True\n",
    "    freeze_text: bool = True\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    epochs: int = 10\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    alpha_domain: float = 0.5\n",
    "    \n",
    "    # Data\n",
    "    k_frames: int = 5\n",
    "    k_audio_chunks: int = 5\n",
    "    sample_rate: int = 16000\n",
    "    image_size: int = 224\n",
    "    max_text_tokens: int = 256\n",
    "    \n",
    "    @classmethod\n",
    "    def from_gpu_memory(cls, gpu_memory_gb: float):\n",
    "        if gpu_memory_gb >= 40:\n",
    "            print(\"üöÄ Using LARGE model configuration\")\n",
    "            return cls(preset=\"large\")\n",
    "        else:\n",
    "            print(\"‚ö° Using SMALL model configuration\")\n",
    "            return cls(\n",
    "                preset=\"small\",\n",
    "                vision_backbone=\"resnet50\",\n",
    "                audio_backbone=\"facebook/wav2vec2-base\",\n",
    "                d_model=256,\n",
    "                n_heads=4,\n",
    "                n_layers=2,\n",
    "                batch_size=4\n",
    "            )\n",
    "\n",
    "# Create config based on GPU\n",
    "config = ModelConfig.from_gpu_memory(gpu_memory_gb)\n",
    "print(f\"\\nüìä Model Config:\")\n",
    "print(f\"  - Preset: {config.preset.upper()}\")\n",
    "print(f\"  - Model dim: {config.d_model}\")\n",
    "print(f\"  - Layers: {config.n_layers}\")\n",
    "print(f\"  - Heads: {config.n_heads}\")\n",
    "print(f\"  - Batch size: {config.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4186e91-fbb7-4a22-96a8-f53712fb51dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRL defined!\n"
     ]
    }
   ],
   "source": [
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    'Domain-Adversarial Training of Neural Networks'\n",
    "    Reverses gradients during backward pass for domain adaptation.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    \"\"\"Wrapper for gradient reversal\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.alpha)\n",
    "    \n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "print(\"GRL defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46aeea41-dadb-452c-a762-c988592209b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All encoders defined!\n"
     ]
    }
   ],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Visual encoder for images/video frames.\n",
    "    Extracts per-frame token embeddings using pretrained vision models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Load backbone\n",
    "        if \"vit\" in config.vision_backbone.lower():\n",
    "            self.backbone = timm.create_model(\n",
    "                config.vision_backbone,\n",
    "                pretrained=config.vision_pretrained,\n",
    "                num_classes=0  # Remove classification head\n",
    "            )\n",
    "            self.feature_dim = self.backbone.num_features\n",
    "        elif \"resnet\" in config.vision_backbone.lower():\n",
    "            self.backbone = timm.create_model(\n",
    "                config.vision_backbone,\n",
    "                pretrained=config.vision_pretrained,\n",
    "                num_classes=0\n",
    "            )\n",
    "            self.feature_dim = self.backbone.num_features\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported vision backbone: {config.vision_backbone}\")\n",
    "        \n",
    "        # Freeze backbone if specified\n",
    "        if config.freeze_vision:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Projection to common dimension\n",
    "        self.projection = nn.Linear(self.feature_dim, config.d_model)\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((config.image_size, config.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Tensor of shape (batch, num_frames, C, H, W) or (batch, C, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            tokens: Tensor of shape (batch, num_tokens, d_model)\n",
    "            available: Boolean indicating if visual data is available\n",
    "        \"\"\"\n",
    "        if images is None or images.numel() == 0:\n",
    "            return None, False\n",
    "        \n",
    "        # Handle single images vs video frames\n",
    "        if images.ndim == 4:\n",
    "            # Single image: (batch, C, H, W)\n",
    "            batch_size = images.size(0)\n",
    "            num_frames = 1\n",
    "            images = images.unsqueeze(1)  # (batch, 1, C, H, W)\n",
    "        else:\n",
    "            # Video frames: (batch, num_frames, C, H, W)\n",
    "            batch_size, num_frames = images.size(0), images.size(1)\n",
    "        \n",
    "        # Reshape to process all frames\n",
    "        images_flat = images.view(batch_size * num_frames, *images.shape[2:])\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.set_grad_enabled(not self.config.freeze_vision):\n",
    "            features = self.backbone(images_flat)  # (batch*num_frames, feature_dim)\n",
    "        \n",
    "        # Project to common dimension\n",
    "        tokens = self.projection(features)  # (batch*num_frames, d_model)\n",
    "        \n",
    "        # Reshape back to (batch, num_frames, d_model)\n",
    "        tokens = tokens.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        return tokens, True\n",
    "\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio encoder using Wav2Vec2 or similar pretrained models.\n",
    "    Extracts audio tokens from waveforms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Load Wav2Vec2 model\n",
    "        try:\n",
    "            self.backbone = Wav2Vec2Model.from_pretrained(config.audio_backbone)\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(config.audio_backbone)\n",
    "            self.feature_dim = self.backbone.config.hidden_size\n",
    "            \n",
    "            # Freeze backbone if specified\n",
    "            if config.freeze_audio:\n",
    "                for param in self.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # Projection to common dimension\n",
    "            self.projection = nn.Linear(self.feature_dim, config.d_model)\n",
    "            self.available = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load audio model: {e}\")\n",
    "            print(\"Using fallback CNN encoder\")\n",
    "            self.available = False\n",
    "            self._build_fallback_encoder(config)\n",
    "    \n",
    "    def _build_fallback_encoder(self, config):\n",
    "        \"\"\"Build simple CNN encoder for audio spectrograms\"\"\"\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=10, stride=5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(8),\n",
    "            nn.Conv1d(64, 128, kernel_size=3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(32)\n",
    "        )\n",
    "        self.projection = nn.Linear(128 * 32, config.d_model)\n",
    "        self.feature_dim = 128 * 32\n",
    "    \n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waveforms: Tensor of shape (batch, num_chunks, samples) or (batch, samples)\n",
    "        \n",
    "        Returns:\n",
    "            tokens: Tensor of shape (batch, num_tokens, d_model)\n",
    "            available: Boolean indicating if audio data is available\n",
    "        \"\"\"\n",
    "        if waveforms is None or waveforms.numel() == 0:\n",
    "            return None, False\n",
    "        \n",
    "        # Handle single waveform vs chunks\n",
    "        if waveforms.ndim == 2:\n",
    "            batch_size = waveforms.size(0)\n",
    "            num_chunks = 1\n",
    "            waveforms = waveforms.unsqueeze(1)  # (batch, 1, samples)\n",
    "        else:\n",
    "            batch_size, num_chunks = waveforms.size(0), waveforms.size(1)\n",
    "        \n",
    "        # Reshape to process all chunks\n",
    "        waveforms_flat = waveforms.view(batch_size * num_chunks, -1)\n",
    "        \n",
    "        # Extract features\n",
    "        if self.available:\n",
    "            with torch.set_grad_enabled(not self.config.freeze_audio):\n",
    "                outputs = self.backbone(waveforms_flat)\n",
    "                features = outputs.last_hidden_state.mean(dim=1)  # Pool over time\n",
    "        else:\n",
    "            # Fallback CNN\n",
    "            waveforms_flat = waveforms_flat.unsqueeze(1)  # Add channel dim\n",
    "            features = self.backbone(waveforms_flat)\n",
    "            features = features.view(batch_size * num_chunks, -1)\n",
    "        \n",
    "        # Project to common dimension\n",
    "        tokens = self.projection(features)  # (batch*num_chunks, d_model)\n",
    "        \n",
    "        # Reshape back\n",
    "        tokens = tokens.view(batch_size, num_chunks, -1)\n",
    "        \n",
    "        return tokens, True\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Text encoder for transcripts using sentence transformers or similar.\n",
    "    Extracts text embeddings from transcripts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Load text model\n",
    "        try:\n",
    "            if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "                self.backbone = SentenceTransformer(config.text_backbone)\n",
    "                self.feature_dim = self.backbone.get_sentence_embedding_dimension()\n",
    "            else:\n",
    "                # Fallback to distilbert\n",
    "                self.backbone = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "                self.feature_dim = 768\n",
    "            \n",
    "            # Freeze backbone if specified\n",
    "            if config.freeze_text:\n",
    "                for param in self.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            # Projection to common dimension\n",
    "            self.projection = nn.Linear(self.feature_dim, config.d_model)\n",
    "            self.available = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load text model: {e}\")\n",
    "            self.available = False\n",
    "            self.feature_dim = config.d_model\n",
    "            self.projection = nn.Identity()\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: List of strings or None\n",
    "        \n",
    "        Returns:\n",
    "            tokens: Tensor of shape (batch, 1, d_model) - pooled text embedding\n",
    "            available: Boolean indicating if text data is available\n",
    "        \"\"\"\n",
    "        if texts is None or len(texts) == 0:\n",
    "            return None, False\n",
    "        \n",
    "        batch_size = len(texts)\n",
    "        \n",
    "        # Extract features\n",
    "        if self.available:\n",
    "            if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "                with torch.set_grad_enabled(not self.config.freeze_text):\n",
    "                    embeddings = self.backbone.encode(\n",
    "                        texts, \n",
    "                        convert_to_tensor=True,\n",
    "                        show_progress_bar=False\n",
    "                    )\n",
    "            else:\n",
    "                # Fallback: use tokenizer + model\n",
    "                inputs = self.tokenizer(\n",
    "                    texts, \n",
    "                    return_tensors='pt', \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=self.config.max_text_tokens\n",
    "                ).to(next(self.backbone.parameters()).device)\n",
    "                \n",
    "                with torch.set_grad_enabled(not self.config.freeze_text):\n",
    "                    outputs = self.backbone(**inputs)\n",
    "                    embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        else:\n",
    "            # Return zeros if not available\n",
    "            device = next(self.projection.parameters()).device\n",
    "            embeddings = torch.zeros(batch_size, self.feature_dim, device=device)\n",
    "        \n",
    "        # Project to common dimension\n",
    "        tokens = self.projection(embeddings)  # (batch, d_model)\n",
    "        \n",
    "        # Add sequence dimension\n",
    "        tokens = tokens.unsqueeze(1)  # (batch, 1, d_model)\n",
    "        \n",
    "        return tokens, True\n",
    "\n",
    "\n",
    "class MetadataEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Metadata encoder for categorical features.\n",
    "    Encodes metadata like uploader, platform, date, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig, \n",
    "                 n_uploaders=100, n_platforms=10, n_date_buckets=12, n_likes_buckets=10):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Categorical embeddings\n",
    "        self.uploader_emb = nn.Embedding(n_uploaders, 64)\n",
    "        self.platform_emb = nn.Embedding(n_platforms, 32)\n",
    "        self.date_emb = nn.Embedding(n_date_buckets, 32)\n",
    "        self.likes_emb = nn.Embedding(n_likes_buckets, 32)\n",
    "        \n",
    "        # MLP to project to common dimension\n",
    "        total_dim = 64 + 32 + 32 + 32\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(total_dim, config.d_model),\n",
    "            nn.LayerNorm(config.d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_model, config.d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, metadata):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metadata: Dict with keys 'uploader', 'platform', 'date', 'likes' (LongTensor)\n",
    "        \n",
    "        Returns:\n",
    "            tokens: Tensor of shape (batch, 1, d_model)\n",
    "            available: Boolean indicating if metadata is available\n",
    "        \"\"\"\n",
    "        if metadata is None or len(metadata) == 0:\n",
    "            return None, False\n",
    "        \n",
    "        # Get embeddings for each field\n",
    "        embs = []\n",
    "        if 'uploader' in metadata:\n",
    "            embs.append(self.uploader_emb(metadata['uploader']))\n",
    "        if 'platform' in metadata:\n",
    "            embs.append(self.platform_emb(metadata['platform']))\n",
    "        if 'date' in metadata:\n",
    "            embs.append(self.date_emb(metadata['date']))\n",
    "        if 'likes' in metadata:\n",
    "            embs.append(self.likes_emb(metadata['likes']))\n",
    "        \n",
    "        if len(embs) == 0:\n",
    "            return None, False\n",
    "        \n",
    "        # Concatenate and project\n",
    "        combined = torch.cat(embs, dim=-1)\n",
    "        tokens = self.mlp(combined)\n",
    "        \n",
    "        # Add sequence dimension\n",
    "        tokens = tokens.unsqueeze(1)  # (batch, 1, d_model)\n",
    "        \n",
    "        return tokens, True\n",
    "\n",
    "print(\"All encoders defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30715d2-a55f-42c2-8cc3-6977722d3cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion transformer defined!\n"
     ]
    }
   ],
   "source": [
    "class CrossModalFusionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-modal fusion using Transformer encoder.\n",
    "    Fuses tokens from all modalities using self-attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Modality embeddings (learned)\n",
    "        self.modality_embeddings = nn.Embedding(4, config.d_model)  # 4 modalities\n",
    "        \n",
    "        # CLS token for pooling\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.d_model))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.n_heads,\n",
    "            dim_feedforward=config.d_model * 4,\n",
    "            dropout=config.dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=config.n_layers,\n",
    "            norm=nn.LayerNorm(config.d_model)\n",
    "        )\n",
    "        \n",
    "        # Modality IDs\n",
    "        self.VISUAL_ID = 0\n",
    "        self.AUDIO_ID = 1\n",
    "        self.TEXT_ID = 2\n",
    "        self.META_ID = 3\n",
    "    \n",
    "    def forward(self, visual_tokens=None, audio_tokens=None, \n",
    "                text_tokens=None, meta_tokens=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            visual_tokens: (batch, n_visual, d_model) or None\n",
    "            audio_tokens: (batch, n_audio, d_model) or None\n",
    "            text_tokens: (batch, n_text, d_model) or None\n",
    "            meta_tokens: (batch, n_meta, d_model) or None\n",
    "            attention_mask: (batch, total_tokens) - True for valid tokens\n",
    "        \n",
    "        Returns:\n",
    "            fused_vector: (batch, d_model) - pooled representation\n",
    "            all_tokens: (batch, total_tokens, d_model) - all output tokens\n",
    "        \"\"\"\n",
    "        batch_size = (visual_tokens.size(0) if visual_tokens is not None \n",
    "                     else audio_tokens.size(0) if audio_tokens is not None\n",
    "                     else text_tokens.size(0) if text_tokens is not None\n",
    "                     else meta_tokens.size(0))\n",
    "        \n",
    "        device = (visual_tokens.device if visual_tokens is not None\n",
    "                 else audio_tokens.device if audio_tokens is not None\n",
    "                 else text_tokens.device if text_tokens is not None\n",
    "                 else meta_tokens.device)\n",
    "        \n",
    "        # Collect all tokens\n",
    "        all_tokens = []\n",
    "        modality_ids = []\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        all_tokens.append(cls_tokens)\n",
    "        # CLS doesn't need modality embedding\n",
    "        \n",
    "        # Add visual tokens\n",
    "        if visual_tokens is not None:\n",
    "            n_visual = visual_tokens.size(1)\n",
    "            visual_mod_emb = self.modality_embeddings(\n",
    "                torch.full((batch_size, n_visual), self.VISUAL_ID, \n",
    "                          dtype=torch.long, device=device)\n",
    "            )\n",
    "            visual_tokens = visual_tokens + visual_mod_emb\n",
    "            all_tokens.append(visual_tokens)\n",
    "        \n",
    "        # Add audio tokens\n",
    "        if audio_tokens is not None:\n",
    "            n_audio = audio_tokens.size(1)\n",
    "            audio_mod_emb = self.modality_embeddings(\n",
    "                torch.full((batch_size, n_audio), self.AUDIO_ID,\n",
    "                          dtype=torch.long, device=device)\n",
    "            )\n",
    "            audio_tokens = audio_tokens + audio_mod_emb\n",
    "            all_tokens.append(audio_tokens)\n",
    "        \n",
    "        # Add text tokens\n",
    "        if text_tokens is not None:\n",
    "            n_text = text_tokens.size(1)\n",
    "            text_mod_emb = self.modality_embeddings(\n",
    "                torch.full((batch_size, n_text), self.TEXT_ID,\n",
    "                          dtype=torch.long, device=device)\n",
    "            )\n",
    "            text_tokens = text_tokens + text_mod_emb\n",
    "            all_tokens.append(text_tokens)\n",
    "        \n",
    "        # Add metadata tokens\n",
    "        if meta_tokens is not None:\n",
    "            n_meta = meta_tokens.size(1)\n",
    "            meta_mod_emb = self.modality_embeddings(\n",
    "                torch.full((batch_size, n_meta), self.META_ID,\n",
    "                          dtype=torch.long, device=device)\n",
    "            )\n",
    "            meta_tokens = meta_tokens + meta_mod_emb\n",
    "            all_tokens.append(meta_tokens)\n",
    "        \n",
    "        # Concatenate all tokens\n",
    "        if len(all_tokens) == 0:\n",
    "            raise ValueError(\"At least one modality must be provided\")\n",
    "        \n",
    "        combined_tokens = torch.cat(all_tokens, dim=1)  # (batch, total_tokens, d_model)\n",
    "        \n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(\n",
    "                batch_size, combined_tokens.size(1),\n",
    "                dtype=torch.bool, device=device\n",
    "            )\n",
    "        \n",
    "        # Convert mask for transformer (True = mask out)\n",
    "        src_key_padding_mask = ~attention_mask\n",
    "        \n",
    "        # Apply transformer\n",
    "        output_tokens = self.transformer(\n",
    "            combined_tokens,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Extract CLS token as fused representation\n",
    "        fused_vector = output_tokens[:, 0, :]  # (batch, d_model)\n",
    "        \n",
    "        return fused_vector, output_tokens\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Domain Discriminator\n",
    "# =============================================================================\n",
    "\n",
    "class DomainDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Domain discriminator for adversarial training.\n",
    "    Classifies the source domain of the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_domains, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, n_domains)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, d_model) - features from encoder\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, n_domains) - domain classification logits\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Classifier\n",
    "# =============================================================================\n",
    "\n",
    "class ClassifierMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary classifier for fake/real detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)  # Binary classification (no sigmoid, use BCEWithLogitsLoss)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, d_model) - fused features\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, 1) - raw logits for fake/real\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"Fusion transformer defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf24eb63-d74a-419d-8085-90bdc9e9e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers defined!\n"
     ]
    }
   ],
   "source": [
    "class DomainDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Domain discriminator for adversarial training.\n",
    "    Classifies the source domain of the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_domains, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, n_domains)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, d_model) - features from encoder\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, n_domains) - domain classification logits\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Classifier\n",
    "# =============================================================================\n",
    "\n",
    "class ClassifierMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary classifier for fake/real detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)  # Binary classification (no sigmoid, use BCEWithLogitsLoss)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, d_model) - fused features\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, 1) - raw logits for fake/real\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"Classifiers defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcec87b3-4b9d-4b4e-91c1-26fdc7a09319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete model defined!\n"
     ]
    }
   ],
   "source": [
    "class MultimodalDeepfakeDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete multimodal deepfake detection model with domain-adversarial training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig, n_domains=5):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Encoders\n",
    "        self.visual_encoder = VisualEncoder(config)\n",
    "        self.audio_encoder = AudioEncoder(config)\n",
    "        self.text_encoder = TextEncoder(config)\n",
    "        self.meta_encoder = MetadataEncoder(config)\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = CrossModalFusionTransformer(config)\n",
    "        \n",
    "        # Gradient Reversal Layer\n",
    "        self.grl = GradientReversalLayer(alpha=config.alpha_domain)\n",
    "        \n",
    "        # Domain discriminator\n",
    "        self.domain_discriminator = DomainDiscriminator(\n",
    "            config.d_model, n_domains, config.dropout\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = ClassifierMLP(config.d_model, config.dropout)\n",
    "    \n",
    "    def forward(self, images=None, audio=None, text=None, metadata=None,\n",
    "                return_domain_logits=True):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            images: (batch, num_frames, C, H, W) or None\n",
    "            audio: (batch, num_chunks, samples) or None\n",
    "            text: List of strings or None\n",
    "            metadata: Dict of categorical features or None\n",
    "            return_domain_logits: Whether to compute domain logits\n",
    "        \n",
    "        Returns:\n",
    "            dict with keys:\n",
    "                - 'logits': (batch, 1) - fake/real classification logits\n",
    "                - 'domain_logits': (batch, n_domains) - domain classification logits\n",
    "                - 'fused_vector': (batch, d_model) - fused representation\n",
    "        \"\"\"\n",
    "        # Encode each modality\n",
    "        visual_tokens, visual_avail = self.visual_encoder(images) if images is not None else (None, False)\n",
    "        audio_tokens, audio_avail = self.audio_encoder(audio) if audio is not None else (None, False)\n",
    "        text_tokens, text_avail = self.text_encoder(text) if text is not None else (None, False)\n",
    "        meta_tokens, meta_avail = self.meta_encoder(metadata) if metadata is not None else (None, False)\n",
    "        \n",
    "        # Fuse modalities\n",
    "        fused_vector, all_tokens = self.fusion(\n",
    "            visual_tokens=visual_tokens if visual_avail else None,\n",
    "            audio_tokens=audio_tokens if audio_avail else None,\n",
    "            text_tokens=text_tokens if text_avail else None,\n",
    "            meta_tokens=meta_tokens if meta_avail else None\n",
    "        )\n",
    "        \n",
    "        # Classification\n",
    "        class_logits = self.classifier(fused_vector)\n",
    "        \n",
    "        # Domain classification with GRL\n",
    "        domain_logits = None\n",
    "        if return_domain_logits:\n",
    "            reversed_features = self.grl(fused_vector)\n",
    "            domain_logits = self.domain_discriminator(reversed_features)\n",
    "        \n",
    "        return {\n",
    "            'logits': class_logits,\n",
    "            'domain_logits': domain_logits,\n",
    "            'fused_vector': fused_vector\n",
    "        }\n",
    "    \n",
    "    def set_grl_alpha(self, alpha):\n",
    "        \"\"\"Update GRL alpha for domain adaptation scheduling\"\"\"\n",
    "        self.grl.set_alpha(alpha)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset Classes\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Complete model defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d312fb8-f3a2-4f7a-9d89-afa6f95b9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMultimodalDataset(Dataset):\n",
    "    \"\"\"Dataset with INTERNAL sampling - no Subset wrapping needed\"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, config, split='train', sample_fraction=0.25):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        self.samples = []\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Loading {split.upper()} split\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load ALL 12 datasets\n",
    "        self._load_deepfake_images()\n",
    "        self._load_faceforensics()\n",
    "        self._load_celebdf()\n",
    "        self._load_kaggle_audio()\n",
    "        self._load_demo_audio()\n",
    "        self._load_fakeavceleb()\n",
    "        self._load_dfd_faces()\n",
    "        self._load_dfd_sequences()\n",
    "        self._load_for_audio()\n",
    "        self._load_140k_faces()\n",
    "        self._load_youtube_faces()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Total loaded: {len(self.samples):,} samples\")\n",
    "        \n",
    "        # Apply intelligent balancing (1:1.33 ratio)\n",
    "        self._apply_intelligent_balancing()\n",
    "        \n",
    "        # STRATIFIED REDUCTION DONE INTERNALLY (not with Subset!)\n",
    "        if sample_fraction < 1.0:\n",
    "            self._stratified_sample(sample_fraction)\n",
    "        \n",
    "        # Print final statistics\n",
    "        self._print_statistics()\n",
    "    \n",
    "    def _stratified_sample(self, fraction):\n",
    "        \"\"\"Reduce dataset size while preserving domain distribution\"\"\"\n",
    "        print(f\"\\n‚öñÔ∏è Stratified sampling to {fraction*100:.0f}% of data...\")\n",
    "        \n",
    "        # Group samples by domain\n",
    "        from collections import defaultdict\n",
    "        domain_samples = defaultdict(list)\n",
    "        for idx, sample in enumerate(self.samples):\n",
    "            domain_samples[sample['domain']].append(idx)\n",
    "        \n",
    "        # Sample from each domain proportionally\n",
    "        selected_indices = []\n",
    "        for domain, indices in domain_samples.items():\n",
    "            n_samples = max(1, int(len(indices) * fraction))\n",
    "            import random\n",
    "            random.seed(42)\n",
    "            sampled = random.sample(indices, n_samples)\n",
    "            selected_indices.extend(sampled)\n",
    "        \n",
    "        # Update samples to only include selected ones\n",
    "        self.samples = [self.samples[i] for i in selected_indices]\n",
    "        \n",
    "        print(f\"  Reduced to {len(self.samples):,} samples\")\n",
    "    \n",
    "    def _apply_intelligent_balancing(self):\n",
    "        \"\"\"Balance to 1:1.33 Real:Fake ratio\"\"\"\n",
    "        real_samples = [s for s in self.samples if s['label'] == 0]\n",
    "        fake_samples = [s for s in self.samples if s['label'] == 1]\n",
    "        \n",
    "        print(f\"\\nüìä Before Balancing:\")\n",
    "        print(f\"  Real: {len(real_samples):,}\")\n",
    "        print(f\"  Fake: {len(fake_samples):,}\")\n",
    "        print(f\"  Ratio: 1:{len(fake_samples)/max(1,len(real_samples)):.2f}\")\n",
    "        \n",
    "        # Target ratio: 1:1.33\n",
    "        target_ratio = 1.33\n",
    "        target_fake_count = int(len(real_samples) * target_ratio)\n",
    "        \n",
    "        # Undersample fakes if too many\n",
    "        if len(fake_samples) > target_fake_count:\n",
    "            print(f\"\\n‚öñÔ∏è Undersampling Fake samples to achieve 1:{target_ratio} ratio\")\n",
    "            import random\n",
    "            random.seed(42)\n",
    "            fake_samples = random.sample(fake_samples, target_fake_count)\n",
    "        \n",
    "        # Combine balanced samples\n",
    "        self.samples = real_samples + fake_samples\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        random.shuffle(self.samples)\n",
    "        \n",
    "        print(f\"\\n‚úÖ After Balancing:\")\n",
    "        print(f\"  Real: {len(real_samples):,}\")\n",
    "        print(f\"  Fake: {len(fake_samples):,}\")\n",
    "        print(f\"  Ratio: 1:{len(fake_samples)/len(real_samples):.2f}\")\n",
    "        print(f\"  Total: {len(self.samples):,}\")\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        if len(self.samples) == 0:\n",
    "            return\n",
    "        \n",
    "        # Count by dataset\n",
    "        dataset_counts = {}\n",
    "        for sample in self.samples:\n",
    "            ds = sample['dataset']\n",
    "            dataset_counts[ds] = dataset_counts.get(ds, 0) + 1\n",
    "        \n",
    "        # Count by type\n",
    "        type_counts = {}\n",
    "        for sample in self.samples:\n",
    "            t = sample['type']\n",
    "            type_counts[t] = type_counts.get(t, 0) + 1\n",
    "        \n",
    "        # Count labels\n",
    "        fake_count = sum(1 for s in self.samples if s['label'] == 1)\n",
    "        real_count = len(self.samples) - fake_count\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä FINAL Dataset Statistics ({self.split}):\")\n",
    "        print(f\"  Total: {len(self.samples):,} samples\")\n",
    "        print(f\"  Real: {real_count:,} | Fake: {fake_count:,}\")\n",
    "        print(f\"  Ratio: 1:{fake_count/max(1,real_count):.2f}\")\n",
    "        print(f\"\\n  By Type:\")\n",
    "        for t, count in type_counts.items():\n",
    "            print(f\"    {t}: {count:,}\")\n",
    "        print(f\"\\n  By Dataset:\")\n",
    "        for ds, count in sorted(dataset_counts.items()):\n",
    "            print(f\"    {ds}: {count:,}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Keep all your existing _load methods and __getitem__ exactly as they are\n",
    "    # ...\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DATASETS - NO SUBSET WRAPPING!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING DATASETS WITH INTERNAL SAMPLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create datasets with 25% sampling done internally\n",
    "train_dataset = EnhancedMultimodalDataset(\n",
    "    data_root=Path('../'),\n",
    "    config=config,\n",
    "    split='train',\n",
    "    sample_fraction=0.25  # 25% of data, sampled internally\n",
    ")\n",
    "\n",
    "test_dataset = EnhancedMultimodalDataset(\n",
    "    data_root=Path('../'),\n",
    "    config=config,\n",
    "    split='test',\n",
    "    sample_fraction=0.25\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets created!\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Test:  {len(test_dataset):,} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# FOCAL LOSS\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "print(\"‚úÖ FocalLoss created\")\n",
    "\n",
    "# ============================================================================\n",
    "# WEIGHTED RANDOM SAMPLER - DIRECT ACCESS\n",
    "# ============================================================================\n",
    "\n",
    "def get_class_weights_direct(dataset):\n",
    "    \"\"\"Get class weights from dataset.samples directly\"\"\"\n",
    "    labels = [s['label'] for s in dataset.samples]\n",
    "    \n",
    "    from collections import Counter\n",
    "    counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    weights = {label: total / count for label, count in counts.items()}\n",
    "    \n",
    "    sample_weights = torch.DoubleTensor([weights[label] for label in labels])\n",
    "    return sample_weights\n",
    "\n",
    "# Create sampler\n",
    "sample_weights = get_class_weights_direct(train_dataset)\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ WeightedRandomSampler created with {len(sample_weights):,} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATALOADERS - SIMPLE & DIRECT (NO SUBSET!)\n",
    "# ============================================================================\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    sampler=sampler,\n",
    "    collate_fn=multimodal_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=multimodal_collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders Created:\")\n",
    "print(f\"   Train batches: {len(train_loader):,}\")\n",
    "print(f\"   Test batches:  {len(test_loader):,}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87925ce-d5f2-41cd-9528-0d2191852815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced dataset loader defined!\n"
     ]
    }
   ],
   "source": [
    "# class EnhancedMultimodalDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     Enhanced dataset that loads ALL available datasets.\n",
    "#     Supports: Images, Audio, Video from 12 major sources.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, data_root, config, split='train'):\n",
    "#         self.data_root = Path(data_root)\n",
    "#         self.config = config\n",
    "#         self.split = split\n",
    "#         self.samples = []\n",
    "        \n",
    "#         print(f\"\\nüìÇ Scanning for datasets in: {data_root}\")\n",
    "#         self._scan_all_datasets()\n",
    "#         print(f\"\\n‚úÖ Loaded {len(self.samples)} samples for {split} split\")\n",
    "#         self._print_statistics()\n",
    "    \n",
    "#     def _scan_all_datasets(self):\n",
    "#         \"\"\"Scan and load all available datasets\"\"\"\n",
    "        \n",
    "#         # 1. Deepfake image detection dataset\n",
    "#         self._load_deepfake_images()\n",
    "        \n",
    "#         # 2. Archive dataset - DISABLED (not available)\n",
    "#         # self._load_archive_dataset()\n",
    "        \n",
    "#         # 3. FaceForensics++\n",
    "#         self._load_faceforensics()\n",
    "        \n",
    "#         # 4. Celeb-DF V2\n",
    "#         self._load_celebdf()\n",
    "        \n",
    "#         # 5. KAGGLE Audio\n",
    "#         self._load_kaggle_audio()\n",
    "        \n",
    "#         # 6. DEMONSTRATION Audio\n",
    "#         self._load_demo_audio()\n",
    "        \n",
    "#         # 7. FakeAVCeleb\n",
    "#         self._load_fakeavceleb()\n",
    "        \n",
    "#         # 8. DFD faces\n",
    "#         self._load_dfd_faces()\n",
    "        \n",
    "#         # 9. DFD sequences\n",
    "#         self._load_dfd_sequences()\n",
    "        \n",
    "#         # 10. FoR Audio Dataset (4 versions)\n",
    "#         self._load_for_audio()\n",
    "        \n",
    "#         # 11. 140k Real and Fake Faces\n",
    "#         self._load_140k_faces()\n",
    "        \n",
    "#         # 12. YouTube Faces videos\n",
    "#         self._load_youtube_faces()\n",
    "        \n",
    "#         # Apply intelligent balancing to achieve 1:2 to 1:2.5 Real:Fake ratio\n",
    "#         if self.split == 'train':\n",
    "#             self._apply_intelligent_balancing()\n",
    "    \n",
    "#     def _load_deepfake_images(self):\n",
    "#         \"\"\"Load Deepfake image detection dataset - ALL SUBFOLDERS\"\"\"\n",
    "#         base = self.data_root / 'Deepfake image detection dataset'\n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó Deepfake Images not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "        \n",
    "#         # Load from train-20250112T065955Z-001/train/\n",
    "#         train_base = base / 'train-20250112T065955Z-001' / 'train'\n",
    "#         if train_base.exists():\n",
    "#             for label_name in ['fake', 'real']:\n",
    "#                 label_dir = train_base / label_name\n",
    "#                 if label_dir.exists():\n",
    "#                     for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "#                         for img in label_dir.glob(ext):\n",
    "#                             self.samples.append({\n",
    "#                                 'path': str(img),\n",
    "#                                 'type': 'image',\n",
    "#                                 'label': 1 if label_name == 'fake' else 0,\n",
    "#                                 'domain': 0,\n",
    "#                                 'dataset': 'DeepfakeImages'\n",
    "#                             })\n",
    "#                             count += 1\n",
    "        \n",
    "#         # Load from test-20250112T065939Z-001/test/\n",
    "#         test_base = base / 'test-20250112T065939Z-001' / 'test'\n",
    "#         if test_base.exists():\n",
    "#             for label_name in ['fake', 'real']:\n",
    "#                 label_dir = test_base / label_name\n",
    "#                 if label_dir.exists():\n",
    "#                     for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "#                         for img in label_dir.glob(ext):\n",
    "#                             self.samples.append({\n",
    "#                                 'path': str(img),\n",
    "#                                 'type': 'image',\n",
    "#                                 'label': 1 if label_name == 'fake' else 0,\n",
    "#                                 'domain': 0,\n",
    "#                                 'dataset': 'DeepfakeImages'\n",
    "#                             })\n",
    "#                             count += 1\n",
    "        \n",
    "#         # Load from Sample_fake_images/\n",
    "#         sample_base = base / 'Sample_fake_images'\n",
    "#         if sample_base.exists():\n",
    "#             for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "#                 for img in sample_base.glob(ext):\n",
    "#                     self.samples.append({\n",
    "#                         'path': str(img),\n",
    "#                         'type': 'image',\n",
    "#                         'label': 1,\n",
    "#                         'domain': 0,\n",
    "#                         'dataset': 'DeepfakeImages'\n",
    "#                     })\n",
    "#                     count += 1\n",
    "        \n",
    "#         print(f\"  ‚úì DeepfakeImages: {count} samples\")\n",
    "    \n",
    "#     def _load_faceforensics(self):\n",
    "#         \"\"\"Load FaceForensics++ dataset\"\"\"\n",
    "#         base = self.data_root / 'FaceForensics++' / 'FaceForensics++_C23'\n",
    "        \n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó FaceForensics++ not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         for manip_type in ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures', 'FaceShifter', 'original']:\n",
    "#             manip_dir = base / manip_type\n",
    "#             if manip_dir.exists():\n",
    "#                 for vid in manip_dir.glob('*.mp4'):\n",
    "#                     self.samples.append({\n",
    "#                         'path': str(vid),\n",
    "#                         'type': 'video',\n",
    "#                         'label': 0 if manip_type == 'original' else 1,\n",
    "#                         'domain': 2,\n",
    "#                         'dataset': 'FaceForensics++'\n",
    "#                     })\n",
    "#                     count += 1\n",
    "#         print(f\"  ‚úì FaceForensics++: {count} samples\")\n",
    "    \n",
    "#     def _load_celebdf(self):\n",
    "#         \"\"\"Load Celeb-DF V2 dataset\"\"\"\n",
    "#         base = self.data_root / 'Celeb V2'\n",
    "        \n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó Celeb-DF V2 not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         for split_type in ['Celeb-synthesis', 'Celeb-real', 'YouTube-real']:\n",
    "#             split_dir = base / split_type\n",
    "#             if split_dir.exists():\n",
    "#                 for vid in split_dir.glob('*.mp4'):\n",
    "#                     self.samples.append({\n",
    "#                         'path': str(vid),\n",
    "#                         'type': 'video',\n",
    "#                         'label': 1 if 'synthesis' in split_type else 0,\n",
    "#                         'domain': 3,\n",
    "#                         'dataset': 'Celeb-DF'\n",
    "#                     })\n",
    "#                     count += 1\n",
    "#         print(f\"  ‚úì Celeb-DF V2: {count} samples\")\n",
    "    \n",
    "#     def _load_kaggle_audio(self):\n",
    "#         \"\"\"Load KAGGLE Audio dataset\"\"\"\n",
    "#         base = self.data_root / 'DeepFake_AudioDataset' / 'KAGGLE' / 'AUDIO'\n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó KAGGLE Audio not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         for label_name in ['FAKE', 'REAL']:\n",
    "#             label_dir = base / label_name\n",
    "#             if label_dir.exists():\n",
    "#                 for audio in label_dir.glob('*.wav'):\n",
    "#                     self.samples.append({\n",
    "#                         'path': str(audio),\n",
    "#                         'type': 'audio',\n",
    "#                         'label': 1 if label_name == 'FAKE' else 0,\n",
    "#                         'domain': 4,\n",
    "#                         'dataset': 'KAGGLE_Audio'\n",
    "#                     })\n",
    "#                     count += 1\n",
    "#         print(f\"  ‚úì KAGGLE Audio: {count} samples\")\n",
    "    \n",
    "#     def _load_demo_audio(self):\n",
    "#         \"\"\"Load DEMONSTRATION Audio\"\"\"\n",
    "#         base = self.data_root / 'DeepFake_AudioDataset' / 'DEMONSTRATION' / 'DEMONSTRATION'\n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó DEMONSTRATION Audio not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         for audio in base.glob('*.mp3'):\n",
    "#             label = 1 if 'to' in audio.stem else 0\n",
    "#             self.samples.append({\n",
    "#                 'path': str(audio),\n",
    "#                 'type': 'audio',\n",
    "#                 'label': label,\n",
    "#                 'domain': 5,\n",
    "#                 'dataset': 'DEMO_Audio'\n",
    "#             })\n",
    "#             count += 1\n",
    "#         print(f\"  ‚úì DEMONSTRATION Audio: {count} samples\")\n",
    "    \n",
    "#     def _load_fakeavceleb(self):\n",
    "#         \"\"\"Load FakeAVCeleb dataset\"\"\"\n",
    "#         base = self.data_root / 'FakeAVCeleb' / 'FakeAVCeleb_v1.2' / 'FakeAVCeleb_v1.2'\n",
    "        \n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó FakeAVCeleb not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         for category in ['FakeVideo-FakeAudio', 'FakeVideo-RealAudio', 'RealVideo-FakeAudio', 'RealVideo-RealAudio']:\n",
    "#             cat_dir = base / category\n",
    "#             if cat_dir.exists():\n",
    "#                 for vid in cat_dir.rglob('*.mp4'):\n",
    "#                     label = 1 if 'Fake' in category else 0\n",
    "#                     self.samples.append({\n",
    "#                         'path': str(vid),\n",
    "#                         'type': 'video',\n",
    "#                         'label': label,\n",
    "#                         'domain': 6,\n",
    "#                         'dataset': 'FakeAVCeleb'\n",
    "#                     })\n",
    "#                     count += 1\n",
    "#         print(f\"  ‚úì FakeAVCeleb: {count} samples\")\n",
    "    \n",
    "#     def _load_dfd_faces(self):\n",
    "#         \"\"\"Load DFD faces (extracted frames)\"\"\"\n",
    "#         base = self.data_root / 'dfd_faces'\n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó DFD Faces not found\")\n",
    "#             return\n",
    "        \n",
    "#         split_dir = base / self.split\n",
    "#         if not split_dir.exists():\n",
    "#             print(f\"  ‚úó DFD Faces {self.split} split not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         for label_name in ['fake', 'real']:\n",
    "#             label_dir = split_dir / label_name\n",
    "#             if label_dir.exists():\n",
    "#                 for img in label_dir.rglob('*.jpg'):\n",
    "#                     self.samples.append({\n",
    "#                         'path': str(img),\n",
    "#                         'type': 'image',\n",
    "#                         'label': 1 if label_name == 'fake' else 0,\n",
    "#                         'domain': 7,\n",
    "#                         'dataset': 'DFD_Faces'\n",
    "#                     })\n",
    "#                     count += 1\n",
    "#                 for img in label_dir.rglob('*.png'):\n",
    "#                     self.samples.append({\n",
    "#                         'path': str(img),\n",
    "#                         'type': 'image',\n",
    "#                         'label': 1 if label_name == 'fake' else 0,\n",
    "#                         'domain': 7,\n",
    "#                         'dataset': 'DFD_Faces'\n",
    "#                     })\n",
    "#                     count += 1\n",
    "#         print(f\"  ‚úì DFD Faces: {count} samples\")\n",
    "    \n",
    "#     def _load_dfd_sequences(self):\n",
    "#         \"\"\"Load DFD sequences\"\"\"\n",
    "#         base = self.data_root / 'DFD'\n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó DFD sequences not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         # Manipulated sequences\n",
    "#         manip_dir = base / 'DFD_manipulated_sequences' / 'DFD_manipulated_sequences'\n",
    "#         if manip_dir.exists():\n",
    "#             for vid in manip_dir.rglob('*.mp4'):\n",
    "#                 self.samples.append({\n",
    "#                     'path': str(vid),\n",
    "#                     'type': 'video',\n",
    "#                     'label': 1,\n",
    "#                     'domain': 8,\n",
    "#                     'dataset': 'DFD_Sequences'\n",
    "#                 })\n",
    "#                 count += 1\n",
    "        \n",
    "#         # Original sequences\n",
    "#         orig_dir = base / 'DFD_original sequences' / 'DFD_original_sequences'\n",
    "#         if orig_dir.exists():\n",
    "#             for vid in orig_dir.rglob('*.mp4'):\n",
    "#                 self.samples.append({\n",
    "#                     'path': str(vid),\n",
    "#                     'type': 'video',\n",
    "#                     'label': 0,\n",
    "#                     'domain': 8,\n",
    "#                     'dataset': 'DFD_Sequences'\n",
    "#                 })\n",
    "#                 count += 1\n",
    "        \n",
    "#         print(f\"  ‚úì DFD sequences: {count} samples\")\n",
    "    \n",
    "#     def _load_for_audio(self):\n",
    "#         \"\"\"Load FoR (Fake-or-Real) Audio Dataset - 4 versions\"\"\"\n",
    "#         base = self.data_root / 'The Fake-or-Real (FoR) Dataset (deepfake audio)'\n",
    "        \n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó FoR Audio not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         versions = {\n",
    "#             'for-norm': 'for-norm/for-norm',\n",
    "#             'for-2sec': 'for-2sec/for-2seconds',\n",
    "#         }\n",
    "        \n",
    "#         for version_name, version_path in versions.items():\n",
    "#             version_base = base / version_path\n",
    "#             if not version_base.exists():\n",
    "#                 continue\n",
    "            \n",
    "#             split_map = {'train': 'training', 'test': 'testing', 'val': 'validation'}\n",
    "#             split_dir = version_base / split_map.get(self.split, 'training')\n",
    "            \n",
    "#             if not split_dir.exists():\n",
    "#                 continue\n",
    "            \n",
    "#             for label_name in ['fake', 'real']:\n",
    "#                 label_dir = split_dir / label_name\n",
    "#                 if label_dir.exists():\n",
    "#                     for ext in ['*.wav', '*.mp3', '*.flac']:\n",
    "#                         for audio in label_dir.glob(ext):\n",
    "#                             self.samples.append({\n",
    "#                                 'path': str(audio),\n",
    "#                                 'type': 'audio',\n",
    "#                                 'label': 1 if label_name == 'fake' else 0,\n",
    "#                                 'domain': 9,\n",
    "#                                 'dataset': f'FoR_Audio_{version_name}'\n",
    "#                             })\n",
    "#                             count += 1\n",
    "        \n",
    "#         print(f\"  ‚úì FoR Audio: {count} samples\")\n",
    "    \n",
    "#     def _load_140k_faces(self):\n",
    "#         \"\"\"Load 140k Real and Fake Faces dataset\"\"\"\n",
    "#         base = self.data_root / '140k Real and Fake Faces' / 'real_vs_fake' / 'real-vs-fake'\n",
    "        \n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó 140k Faces not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         split_map = {'train': 'train', 'test': 'test', 'val': 'valid'}\n",
    "#         split_dir = base / split_map.get(self.split, 'train')\n",
    "        \n",
    "#         if not split_dir.exists():\n",
    "#             print(f\"  ‚úó 140k Faces {self.split} split not found\")\n",
    "#             return\n",
    "        \n",
    "#         for label_name in ['fake', 'real']:\n",
    "#             label_dir = split_dir / label_name\n",
    "#             if label_dir.exists():\n",
    "#                 for ext in ['*.jpg', '*.png', '*.jpeg']:\n",
    "#                     for img in label_dir.glob(ext):\n",
    "#                         self.samples.append({\n",
    "#                             'path': str(img),\n",
    "#                             'type': 'image',\n",
    "#                             'label': 1 if label_name == 'fake' else 0,\n",
    "#                             'domain': 10,\n",
    "#                             'dataset': '140k_Faces'\n",
    "#                         })\n",
    "#                         count += 1\n",
    "        \n",
    "#         print(f\"  ‚úì 140k Faces: {count} samples\")\n",
    "    \n",
    "#     def _load_youtube_faces(self):\n",
    "#         \"\"\"Load YouTube Faces Dataset with Facial Keypoints\"\"\"\n",
    "#         base = self.data_root / 'YouTube Faces With Facial Keypoints'\n",
    "        \n",
    "#         if not base.exists():\n",
    "#             print(f\"  ‚úó YouTube Faces not found\")\n",
    "#             return\n",
    "        \n",
    "#         count = 0\n",
    "#         for folder_num in range(1, 5):\n",
    "#             folder = base / f'youtube_faces_with_keypoints_full_{folder_num}' / f'youtube_faces_with_keypoints_full_{folder_num}'\n",
    "#             if folder.exists():\n",
    "#                 for npz_file in folder.glob('*.npz'):\n",
    "#                     self.samples.append({\n",
    "#                         'path': str(npz_file),\n",
    "#                         'type': 'video',\n",
    "#                         'label': 0,\n",
    "#                         'domain': 11,\n",
    "#                         'dataset': 'YouTube_Faces'\n",
    "#                     })\n",
    "#                     count += 1\n",
    "        \n",
    "#         print(f\"  ‚úì YouTube Faces: {count} samples (REAL videos - critical for balancing!)\")\n",
    "    \n",
    "#     def _apply_intelligent_balancing(self):\n",
    "#         \"\"\"Apply intelligent balancing to achieve 1:2 to 1:2.5 Real:Fake ratio\"\"\"\n",
    "#         real_samples = [s for s in self.samples if s['label'] == 0]\n",
    "#         fake_samples = [s for s in self.samples if s['label'] == 1]\n",
    "        \n",
    "#         print(f\"\\nüìä Before Balancing:\")\n",
    "#         print(f\"  Real: {len(real_samples):,}\")\n",
    "#         print(f\"  Fake: {len(fake_samples):,}\")\n",
    "#         print(f\"  Ratio: 1:{len(fake_samples)/len(real_samples):.2f}\")\n",
    "        \n",
    "#         # Target ratio: 1:2.25 (middle of 1:2 to 1:2.5)\n",
    "#         target_ratio = 2.25\n",
    "#         target_fake_count = int(len(real_samples) * target_ratio)\n",
    "        \n",
    "#         # If we have too many fakes, undersample\n",
    "#         if len(fake_samples) > target_fake_count:\n",
    "#             print(f\"\\n‚öñÔ∏è Undersampling Fake samples to achieve 1:{target_ratio} ratio\")\n",
    "#             from sklearn.utils import resample\n",
    "#             fake_samples = resample(fake_samples, \n",
    "#                                    n_samples=target_fake_count,\n",
    "#                                    random_state=42,\n",
    "#                                    replace=False)\n",
    "        \n",
    "#         # Combine balanced samples\n",
    "#         self.samples = real_samples + fake_samples\n",
    "        \n",
    "#         print(f\"\\n‚úÖ After Balancing:\")\n",
    "#         print(f\"  Real: {len(real_samples):,}\")\n",
    "#         print(f\"  Fake: {len(fake_samples):,}\")\n",
    "#         print(f\"  Ratio: 1:{len(fake_samples)/len(real_samples):.2f}\")\n",
    "#         print(f\"  Total: {len(self.samples):,}\")\n",
    "    \n",
    "#     def _print_statistics(self):\n",
    "#         \"\"\"Print dataset statistics\"\"\"\n",
    "#         if len(self.samples) == 0:\n",
    "#             return\n",
    "        \n",
    "#         # Count by dataset\n",
    "#         dataset_counts = {}\n",
    "#         for sample in self.samples:\n",
    "#             ds = sample['dataset']\n",
    "#             dataset_counts[ds] = dataset_counts.get(ds, 0) + 1\n",
    "        \n",
    "#         # Count by type\n",
    "#         type_counts = {}\n",
    "#         for sample in self.samples:\n",
    "#             t = sample['type']\n",
    "#             type_counts[t] = type_counts.get(t, 0) + 1\n",
    "        \n",
    "#         # Count labels\n",
    "#         fake_count = sum(1 for s in self.samples if s['label'] == 1)\n",
    "#         real_count = len(self.samples) - fake_count\n",
    "        \n",
    "#         print(f\"\\nüìä Dataset Statistics:\")\n",
    "#         print(f\"  Total: {len(self.samples)} samples\")\n",
    "#         print(f\"  Real: {real_count} | Fake: {fake_count}\")\n",
    "#         print(f\"\\n  By Type:\")\n",
    "#         for t, count in type_counts.items():\n",
    "#             print(f\"    {t}: {count}\")\n",
    "#         print(f\"\\n  By Dataset:\")\n",
    "#         for ds, count in sorted(dataset_counts.items()):\n",
    "#             print(f\"    {ds}: {count}\")\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         sample = self.samples[idx]\n",
    "        \n",
    "#         # Load data based on type\n",
    "#         if sample['type'] == 'image':\n",
    "#             image = self._load_image(sample['path'])\n",
    "#             return {\n",
    "#                 'image': image,\n",
    "#                 'audio': None,\n",
    "#                 'text': None,\n",
    "#                 'metadata': None,\n",
    "#                 'label': sample['label'],\n",
    "#                 'domain': sample['domain']\n",
    "#             }\n",
    "#         elif sample['type'] == 'audio':\n",
    "#             audio = self._load_audio(sample['path'])\n",
    "#             return {\n",
    "#                 'image': None,\n",
    "#                 'audio': audio,\n",
    "#                 'text': None,\n",
    "#                 'metadata': None,\n",
    "#                 'label': sample['label'],\n",
    "#                 'domain': sample['domain']\n",
    "#             }\n",
    "#         elif sample['type'] == 'video':\n",
    "#             # For videos, extract first frame for now\n",
    "#             image = self._load_video_frame(sample['path'])\n",
    "#             return {\n",
    "#                 'image': image,\n",
    "#                 'audio': None,\n",
    "#                 'text': None,\n",
    "#                 'metadata': None,\n",
    "#                 'label': sample['label'],\n",
    "#                 'domain': sample['domain']\n",
    "#             }\n",
    "    \n",
    "#     def _load_image(self, path):\n",
    "#         try:\n",
    "#             img = cv2.imread(path)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#             img = cv2.resize(img, (self.config.image_size, self.config.image_size))\n",
    "#             img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "#             mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "#             std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "#             img = (img - mean) / std\n",
    "#             return img\n",
    "#         except:\n",
    "#             return torch.zeros(3, self.config.image_size, self.config.image_size)\n",
    "    \n",
    "#     def _load_audio(self, path):\n",
    "#         try:\n",
    "#             waveform, sr = librosa.load(path, sr=self.config.sample_rate, duration=10)\n",
    "#             target_length = self.config.sample_rate * 10\n",
    "#             if len(waveform) < target_length:\n",
    "#                 waveform = np.pad(waveform, (0, target_length - len(waveform)))\n",
    "#             else:\n",
    "#                 waveform = waveform[:target_length]\n",
    "#             return torch.from_numpy(waveform).float()\n",
    "#         except:\n",
    "#             return torch.zeros(self.config.sample_rate * 10)\n",
    "    \n",
    "#     def _load_video_frame(self, path):\n",
    "#         try:\n",
    "#             cap = cv2.VideoCapture(path)\n",
    "#             ret, frame = cap.read()\n",
    "#             if ret:\n",
    "#                 frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#                 frame = cv2.resize(frame, (self.config.image_size, self.config.image_size))\n",
    "#                 frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "#                 mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "#                 std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "#                 frame = (frame - mean) / std\n",
    "#                 cap.release()\n",
    "#                 return frame\n",
    "#             else:\n",
    "#                 cap.release()\n",
    "#                 return torch.zeros(3, self.config.image_size, self.config.image_size)\n",
    "#         except:\n",
    "#             return torch.zeros(3, self.config.image_size, self.config.image_size)\n",
    "\n",
    "# print(\"‚úÖ Enhanced dataset loader defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666f5cc6-919e-4bbd-b6a5-5b630cbd6f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ YouTube Faces .npz handler defined!\n"
     ]
    }
   ],
   "source": [
    "# Add this method to EnhancedMultimodalDataset class to handle .npz files\n",
    "\n",
    "def _load_youtube_npz(self, path):\n",
    "    \"\"\"Load YouTube Faces .npz file containing video frames\"\"\"\n",
    "    try:\n",
    "        # Load .npz file\n",
    "        data = np.load(path)\n",
    "        \n",
    "        # YouTube Faces .npz contains 'colorImages' key with video frames\n",
    "        if 'colorImages' in data:\n",
    "            frames = data['colorImages']\n",
    "            \n",
    "            # Select first frame or random frame\n",
    "            if len(frames) > 0:\n",
    "                frame_idx = 0  # or: np.random.randint(0, len(frames))\n",
    "                frame = frames[frame_idx]\n",
    "                \n",
    "                # Convert to RGB if needed\n",
    "                if frame.shape[-1] != 3:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "                # Resize and normalize\n",
    "                frame = cv2.resize(frame, (self.config.image_size, self.config.image_size))\n",
    "                frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                frame = (frame - mean) / std\n",
    "                return frame\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YouTube .npz: {e}\")\n",
    "        pass\n",
    "    \n",
    "    return torch.zeros(3, self.config.image_size, self.config.image_size)\n",
    "\n",
    "print(\"‚úÖ YouTube Faces .npz handler defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87335d10-ae17-4042-96e6-d3bb01c91e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collate function defined!\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate for variable modalities\"\"\"\n",
    "    images, audios, texts, metadatas = [], [], [], []\n",
    "    labels, domains = [], []\n",
    "    \n",
    "    for item in batch:\n",
    "        # Always append labels and domains\n",
    "        labels.append(item['label'])\n",
    "        domains.append(item['domain'])\n",
    "        \n",
    "        # Append modality data (use zeros if not available)\n",
    "        if item['image'] is not None:\n",
    "            images.append(item['image'])\n",
    "        else:\n",
    "            # Add zero tensor as placeholder\n",
    "            images.append(torch.zeros(3, 224, 224))\n",
    "            \n",
    "        if item['audio'] is not None:\n",
    "            audios.append(item['audio'])\n",
    "        else:\n",
    "            # Add zero tensor as placeholder\n",
    "            audios.append(torch.zeros(16000 * 10))\n",
    "    \n",
    "    return {\n",
    "        'images': torch.stack(images) if images else None,\n",
    "        'audio': torch.stack(audios) if audios else None,\n",
    "        'text': None,\n",
    "        'metadata': None,\n",
    "        'labels': torch.tensor(labels, dtype=torch.float32),\n",
    "        'domains': torch.tensor(domains, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Collate function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed1af1b-1587-4b8f-8ce6-2563dc7e8410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scaler, config, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Update GRL alpha\n",
    "    progress = epoch / config.epochs\n",
    "    alpha = config.alpha_domain * (2 / (1 + np.exp(-10 * progress)) - 1)\n",
    "    model.set_grl_alpha(alpha)\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}')\n",
    "    for batch in pbar:\n",
    "        images = batch['images'].to(device) if batch['images'] is not None else None\n",
    "        audio = batch['audio'].to(device) if batch['audio'] is not None else None\n",
    "        labels = batch['labels'].to(device)\n",
    "        domains = batch['domains'].to(device)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images=images, audio=audio, text=None, metadata=None)\n",
    "            cls_loss = F.binary_cross_entropy_with_logits(outputs['logits'].squeeze(), labels)\n",
    "            dom_loss = F.cross_entropy(outputs['domain_logits'], domains) if outputs['domain_logits'] is not None else 0\n",
    "            loss = cls_loss + alpha * dom_loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(outputs['logits']) > 0.5).float()\n",
    "        correct += (preds.squeeze() == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': total_loss/len(pbar), 'acc': 100.*correct/total})\n",
    "    \n",
    "    return total_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            images = batch['images'].to(device) if batch['images'] is not None else None\n",
    "            audio = batch['audio'].to(device) if batch['audio'] is not None else None\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(images=images, audio=audio, text=None, metadata=None, return_domain_logits=False)\n",
    "            preds = (torch.sigmoid(outputs['logits']) > 0.5).float()\n",
    "            correct += (preds.squeeze() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = 100. * correct / total\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0) * 100\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0) * 100\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0) * 100\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "print(\"Training functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caae97a6-c07e-4c6b-90b3-e7849dca46e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class balancing utilities defined (OPTIMIZED VERSION)!\n",
      "   - FocalLoss: Handles hard examples\n",
      "   - get_class_weights: For balanced sampling\n",
      "   - calculate_pos_weight: For weighted BCE loss\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CLASS BALANCING UTILITIES\n",
    "# ===========================\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling class imbalance.\n",
    "    Focuses on hard-to-classify examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "def get_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    Calculate class weights FAST by accessing metadata directly.\n",
    "    \"\"\"\n",
    "    # ‚úÖ FAST: Access labels from metadata (no file loading)\n",
    "    labels = [s['label'] for s in dataset.samples]\n",
    "    labels_array = np.array(labels)\n",
    "    class_counts = np.bincount(labels_array.astype(int))\n",
    "    \n",
    "    print(f\"\\nüìä Class Distribution:\")\n",
    "    print(f\"   Real (0): {class_counts[0]:,} samples\")\n",
    "    print(f\"   Fake (1): {class_counts[1]:,} samples\")\n",
    "    print(f\"   Imbalance Ratio: {class_counts[1]/class_counts[0]:.2f}:1\")\n",
    "    \n",
    "    # Calculate weights (inverse frequency)\n",
    "    weights = 1. / class_counts\n",
    "    sample_weights = [weights[int(label)] for label in labels]\n",
    "    \n",
    "    return torch.DoubleTensor(sample_weights)\n",
    "\n",
    "def calculate_pos_weight(dataset):\n",
    "    \"\"\"\n",
    "    Calculate pos_weight FAST from metadata.\n",
    "    \"\"\"\n",
    "    # ‚úÖ FAST: Access labels from metadata\n",
    "    labels = [s['label'] for s in dataset.samples]\n",
    "    labels_array = np.array(labels)\n",
    "    class_counts = np.bincount(labels_array.astype(int))\n",
    "    \n",
    "    num_real = class_counts[0]\n",
    "    num_fake = class_counts[1]\n",
    "    pos_weight = num_real / num_fake\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Pos Weight for BCE Loss: {pos_weight:.4f}\")\n",
    "    \n",
    "    return torch.tensor([pos_weight])\n",
    "\n",
    "print(\"‚úÖ Class balancing utilities defined (OPTIMIZED VERSION)!\")\n",
    "print(\"   - FocalLoss: Handles hard examples\")\n",
    "print(\"   - get_class_weights: For balanced sampling\")\n",
    "print(\"   - calculate_pos_weight: For weighted BCE loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe58faf1-5113-45d8-a782-a7541ced64dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING & BALANCING DATASETS\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading datasets...\n",
      "\n",
      "üìÇ Scanning for datasets in: ../\n",
      "  ‚úì DeepfakeImages: 978 samples\n",
      "  ‚úì FaceForensics++: 6000 samples\n",
      "  ‚úì Celeb-DF V2: 6529 samples\n",
      "  ‚úì KAGGLE Audio: 64 samples\n",
      "  ‚úó DEMONSTRATION Audio not found\n",
      "  ‚úì FakeAVCeleb: 21560 samples\n",
      "  ‚úì DFD Faces: 7808 samples\n",
      "  ‚úì DFD sequences: 3068 samples\n",
      "  ‚úì FoR Audio: 67824 samples\n",
      "  ‚úì 140k Faces: 100000 samples\n",
      "  ‚úì YouTube Faces: 2194 samples (REAL videos - critical for balancing!)\n",
      "\n",
      "üìä Before Balancing:\n",
      "  Real: 92,659\n",
      "  Fake: 123,366\n",
      "  Ratio: 1:1.33\n",
      "\n",
      "‚úÖ After Balancing:\n",
      "  Real: 92,659\n",
      "  Fake: 123,366\n",
      "  Ratio: 1:1.33\n",
      "  Total: 216,025\n",
      "\n",
      "‚úÖ Loaded 216025 samples for train split\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "  Total: 216025 samples\n",
      "  Real: 92659 | Fake: 123366\n",
      "\n",
      "  By Type:\n",
      "    image: 108786\n",
      "    video: 39351\n",
      "    audio: 67888\n",
      "\n",
      "  By Dataset:\n",
      "    140k_Faces: 100000\n",
      "    Celeb-DF: 6529\n",
      "    DFD_Faces: 7808\n",
      "    DFD_Sequences: 3068\n",
      "    DeepfakeImages: 978\n",
      "    FaceForensics++: 6000\n",
      "    FakeAVCeleb: 21560\n",
      "    FoR_Audio_for-2sec: 13956\n",
      "    FoR_Audio_for-norm: 53868\n",
      "    KAGGLE_Audio: 64\n",
      "    YouTube_Faces: 2194\n",
      "\n",
      "üìÇ Scanning for datasets in: ../\n",
      "  ‚úì DeepfakeImages: 978 samples\n",
      "  ‚úì FaceForensics++: 6000 samples\n",
      "  ‚úì Celeb-DF V2: 6529 samples\n",
      "  ‚úì KAGGLE Audio: 64 samples\n",
      "  ‚úó DEMONSTRATION Audio not found\n",
      "  ‚úì FakeAVCeleb: 21560 samples\n",
      "  ‚úì DFD Faces: 2448 samples\n",
      "  ‚úì DFD sequences: 3068 samples\n",
      "  ‚úì FoR Audio: 5722 samples\n",
      "  ‚úì 140k Faces: 20000 samples\n",
      "  ‚úì YouTube Faces: 2194 samples (REAL videos - critical for balancing!)\n",
      "\n",
      "‚úÖ Loaded 68563 samples for test split\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "  Total: 68563 samples\n",
      "  Real: 19004 | Fake: 49559\n",
      "\n",
      "  By Type:\n",
      "    image: 23426\n",
      "    video: 39351\n",
      "    audio: 5786\n",
      "\n",
      "  By Dataset:\n",
      "    140k_Faces: 20000\n",
      "    Celeb-DF: 6529\n",
      "    DFD_Faces: 2448\n",
      "    DFD_Sequences: 3068\n",
      "    DeepfakeImages: 978\n",
      "    FaceForensics++: 6000\n",
      "    FakeAVCeleb: 21560\n",
      "    FoR_Audio_for-2sec: 1088\n",
      "    FoR_Audio_for-norm: 4634\n",
      "    KAGGLE_Audio: 64\n",
      "    YouTube_Faces: 2194\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è REDUCING DATASET SIZE (STRATIFIED)\n",
      "============================================================\n",
      "\n",
      "üìä Training Set Reduction:\n",
      "   DeepfakeImages: 978 ‚Üí 244\n",
      "   FaceForensics++: 6,000 ‚Üí 1,500\n",
      "   Celeb-DF: 6,529 ‚Üí 1,632\n",
      "   KAGGLE_Audio: 64 ‚Üí 16\n",
      "   FakeAVCeleb: 21,560 ‚Üí 5,390\n",
      "   DFD_Faces: 7,808 ‚Üí 1,952\n",
      "   FoR_Audio_for-norm: 53,868 ‚Üí 13,467\n",
      "   FoR_Audio_for-2sec: 13,956 ‚Üí 3,489\n",
      "   140k_Faces: 100,000 ‚Üí 25,000\n",
      "   YouTube_Faces: 2,194 ‚Üí 548\n",
      "   DFD_Sequences: 3,068 ‚Üí 767\n",
      "\n",
      "   Total: 216,025 ‚Üí 54,005 samples\n",
      "\n",
      "üìä Test Set Reduction:\n",
      "   DeepfakeImages: 978 ‚Üí 244\n",
      "   FaceForensics++: 6,000 ‚Üí 1,500\n",
      "   Celeb-DF: 6,529 ‚Üí 1,632\n",
      "   KAGGLE_Audio: 64 ‚Üí 16\n",
      "   FakeAVCeleb: 21,560 ‚Üí 5,390\n",
      "   DFD_Faces: 2,448 ‚Üí 612\n",
      "   DFD_Sequences: 3,068 ‚Üí 767\n",
      "   FoR_Audio_for-norm: 4,634 ‚Üí 1,158\n",
      "   FoR_Audio_for-2sec: 1,088 ‚Üí 272\n",
      "   140k_Faces: 20,000 ‚Üí 5,000\n",
      "   YouTube_Faces: 2,194 ‚Üí 548\n",
      "\n",
      "   Total: 68,563 ‚Üí 17,139 samples\n",
      "\n",
      "‚öñÔ∏è Training Set Balancing:\n",
      "  Before: Real=23,006, Fake=30,999, Ratio=1:1.35\n",
      "  After:  Real=23,006, Fake=30,597, Ratio=1:1.33\n",
      "  Total:  53,603 samples\n",
      "  ‚úÖ Training Set balanced!\n",
      "\n",
      "‚öñÔ∏è Test Set Balancing:\n",
      "  Before: Real=4,702, Fake=12,437, Ratio=1:2.65\n",
      "  After:  Real=4,702, Fake=6,253, Ratio=1:1.33\n",
      "  Total:  10,955 samples\n",
      "  ‚úÖ Test Set balanced!\n",
      "\n",
      "‚úÖ Both datasets reduced AND balanced!\n",
      "‚ö° Estimated time: ~15-20 min training + 2-3 min eval per epoch\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîÑ Setting up balanced sampling for training...\n",
      "\n",
      "üìä Class Distribution:\n",
      "   Real (0): 23,006 samples\n",
      "   Fake (1): 30,597 samples\n",
      "   Imbalance Ratio: 1.33:1\n",
      "\n",
      "‚öñÔ∏è Pos Weight for BCE Loss: 0.7519\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED DATALOADER SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìä Training Set:\n",
      "  Total samples: 53,603\n",
      "  Batches per epoch: 6,700\n",
      "  Batch size: 8\n",
      "  Sampling: WeightedRandomSampler (balanced)\n",
      "\n",
      "üìä Test Set:\n",
      "  Total samples: 10,955\n",
      "  Batches: 1,370\n",
      "\n",
      "üéØ Loss Configuration:\n",
      "  Loss Function: Focal Loss\n",
      "  Alpha (Œ±): 0.75, Gamma (Œ≥): 2.0\n",
      "  pos_weight: 0.7519\n",
      "\n",
      "‚úÖ GUARANTEED 1:1.33 ratio in BOTH train and test!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# # ===========================\n",
    "# # CREATE DATASETS WITH AUTOMATIC BALANCING\n",
    "# # ===========================\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"LOADING & BALANCING DATASETS\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Create datasets\n",
    "# print(\"\\nüìÇ Loading datasets...\")\n",
    "# train_dataset = EnhancedMultimodalDataset('../', config, split='train')\n",
    "# test_dataset = EnhancedMultimodalDataset('../', config, split='test')\n",
    "\n",
    "# # ===========================\n",
    "# # STRATIFIED DATASET REDUCTION (25% FROM EACH DATASET) - DO THIS FIRST!\n",
    "# # ===========================\n",
    "\n",
    "# from torch.utils.data import Subset\n",
    "# import random\n",
    "# from collections import defaultdict\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"‚ö†Ô∏è REDUCING DATASET SIZE (STRATIFIED)\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # ========== REDUCE TRAINING SET ==========\n",
    "# print(\"\\nüìä Training Set Reduction:\")\n",
    "\n",
    "# # Group samples by dataset\n",
    "# dataset_samples = defaultdict(list)\n",
    "# for idx, sample in enumerate(train_dataset.samples):\n",
    "#     dataset_samples[sample['dataset']].append(idx)\n",
    "\n",
    "# # Take 25% from EACH dataset\n",
    "# selected_indices = []\n",
    "# for dataset_name, indices in dataset_samples.items():\n",
    "#     n_samples = max(1, len(indices) // 4)\n",
    "#     random.seed(42)\n",
    "#     selected = random.sample(indices, n_samples)\n",
    "#     selected_indices.extend(selected)\n",
    "#     print(f\"   {dataset_name}: {len(indices):,} ‚Üí {len(selected):,}\")\n",
    "\n",
    "# original_train_size = len(train_dataset.samples)\n",
    "# train_dataset = Subset(train_dataset, selected_indices)\n",
    "\n",
    "# print(f\"\\n   Total: {original_train_size:,} ‚Üí {len(train_dataset):,} samples\")\n",
    "\n",
    "# # ========== REDUCE TEST SET ==========\n",
    "# print(\"\\nüìä Test Set Reduction:\")\n",
    "\n",
    "# # Group test samples by dataset\n",
    "# test_dataset_samples = defaultdict(list)\n",
    "# for idx, sample in enumerate(test_dataset.samples):\n",
    "#     test_dataset_samples[sample['dataset']].append(idx)\n",
    "\n",
    "# # Take 25% from EACH dataset\n",
    "# test_selected_indices = []\n",
    "# for dataset_name, indices in test_dataset_samples.items():\n",
    "#     n_samples = max(1, len(indices) // 4)\n",
    "#     random.seed(42)\n",
    "#     selected = random.sample(indices, n_samples)\n",
    "#     test_selected_indices.extend(selected)\n",
    "#     print(f\"   {dataset_name}: {len(indices):,} ‚Üí {len(selected):,}\")\n",
    "\n",
    "# original_test_size = len(test_dataset.samples)\n",
    "# test_dataset = Subset(test_dataset, test_selected_indices)\n",
    "\n",
    "# print(f\"\\n   Total: {original_test_size:,} ‚Üí {len(test_dataset):,} samples\")\n",
    "\n",
    "# # ===========================\n",
    "# # NOW BALANCE BOTH SETS TO 1:1.33 RATIO\n",
    "# # ===========================\n",
    "\n",
    "# def balance_subset_dataset(subset_dataset, target_ratio=1.33, name=\"Dataset\"):\n",
    "#     \"\"\"Balance a Subset dataset to target ratio\"\"\"\n",
    "#     base_dataset = subset_dataset.dataset\n",
    "#     indices = subset_dataset.indices\n",
    "    \n",
    "#     # Get labels for subset indices\n",
    "#     labels = [base_dataset.samples[i]['label'] for i in indices]\n",
    "    \n",
    "#     # Separate indices by label\n",
    "#     real_indices = [idx for idx, label in zip(indices, labels) if label == 0]\n",
    "#     fake_indices = [idx for idx, label in zip(indices, labels) if label == 1]\n",
    "    \n",
    "#     current_ratio = len(fake_indices) / len(real_indices) if len(real_indices) > 0 else 0\n",
    "    \n",
    "#     print(f\"\\n‚öñÔ∏è {name} Balancing:\")\n",
    "#     print(f\"  Before: Real={len(real_indices):,}, Fake={len(fake_indices):,}, Ratio=1:{current_ratio:.2f}\")\n",
    "    \n",
    "#     # Undersample fakes to match target ratio\n",
    "#     target_fake_count = int(len(real_indices) * target_ratio)\n",
    "    \n",
    "#     if len(fake_indices) > target_fake_count:\n",
    "#         random.seed(42)\n",
    "#         fake_indices = random.sample(fake_indices, target_fake_count)\n",
    "    \n",
    "#     # Combine and create new subset\n",
    "#     balanced_indices = real_indices + fake_indices\n",
    "#     random.shuffle(balanced_indices)\n",
    "    \n",
    "#     # Create new Subset with balanced indices\n",
    "#     balanced_subset = Subset(base_dataset, balanced_indices)\n",
    "    \n",
    "#     print(f\"  After:  Real={len(real_indices):,}, Fake={len(fake_indices):,}, Ratio=1:{target_ratio:.2f}\")\n",
    "#     print(f\"  Total:  {len(balanced_subset):,} samples\")\n",
    "#     print(f\"  ‚úÖ {name} balanced!\")\n",
    "    \n",
    "#     return balanced_subset\n",
    "\n",
    "# # Balance both datasets\n",
    "# train_dataset = balance_subset_dataset(train_dataset, target_ratio=1.33, name=\"Training Set\")\n",
    "# test_dataset = balance_subset_dataset(test_dataset, target_ratio=1.33, name=\"Test Set\")\n",
    "\n",
    "# print(f\"\\n‚úÖ Both datasets reduced AND balanced!\")\n",
    "# print(f\"‚ö° Estimated time: ~15-20 min training + 2-3 min eval per epoch\")\n",
    "# print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# # ===========================\n",
    "# # SUBSET-AWARE HELPER FUNCTIONS\n",
    "# # ===========================\n",
    "\n",
    "# def get_class_weights_from_subset(subset_dataset):\n",
    "#     \"\"\"Calculate class weights from a Subset object\"\"\"\n",
    "#     base_dataset = subset_dataset.dataset\n",
    "#     indices = subset_dataset.indices\n",
    "    \n",
    "#     labels = [base_dataset.samples[i]['label'] for i in indices]\n",
    "#     labels_array = np.array(labels)\n",
    "#     class_counts = np.bincount(labels_array.astype(int))\n",
    "    \n",
    "#     print(f\"\\nüìä Class Distribution:\")\n",
    "#     print(f\"   Real (0): {class_counts[0]:,} samples\")\n",
    "#     print(f\"   Fake (1): {class_counts[1]:,} samples\")\n",
    "#     print(f\"   Imbalance Ratio: {class_counts[1]/class_counts[0]:.2f}:1\")\n",
    "    \n",
    "#     weights = 1. / class_counts\n",
    "#     sample_weights = [weights[int(label)] for label in labels]\n",
    "    \n",
    "#     return torch.DoubleTensor(sample_weights)\n",
    "\n",
    "# def calculate_pos_weight_from_subset(subset_dataset):\n",
    "#     \"\"\"Calculate pos_weight from a Subset object\"\"\"\n",
    "#     base_dataset = subset_dataset.dataset\n",
    "#     indices = subset_dataset.indices\n",
    "    \n",
    "#     labels = [base_dataset.samples[i]['label'] for i in indices]\n",
    "#     labels_array = np.array(labels)\n",
    "#     class_counts = np.bincount(labels_array.astype(int))\n",
    "    \n",
    "#     num_real = class_counts[0]\n",
    "#     num_fake = class_counts[1]\n",
    "#     pos_weight = num_real / num_fake\n",
    "    \n",
    "#     print(f\"\\n‚öñÔ∏è Pos Weight for BCE Loss: {pos_weight:.4f}\")\n",
    "    \n",
    "#     return torch.tensor([pos_weight])\n",
    "\n",
    "# # ===========================\n",
    "# # CREATE DATALOADERS\n",
    "# # ===========================\n",
    "\n",
    "# import gc\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "# print(\"\\nüîÑ Setting up balanced sampling for training...\")\n",
    "\n",
    "# sample_weights = get_class_weights_from_subset(train_dataset)\n",
    "\n",
    "# sampler = WeightedRandomSampler(\n",
    "#     sample_weights, \n",
    "#     num_samples=len(train_dataset),\n",
    "#     replacement=True\n",
    "# )\n",
    "\n",
    "# pos_weight = calculate_pos_weight_from_subset(train_dataset).to(device)\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=config.batch_size, \n",
    "#     sampler=sampler,\n",
    "#     collate_fn=collate_fn, \n",
    "#     num_workers=0,\n",
    "#     pin_memory=True,\n",
    "#     drop_last=True\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     test_dataset, \n",
    "#     batch_size=config.batch_size, \n",
    "#     shuffle=False,\n",
    "#     collate_fn=collate_fn, \n",
    "#     num_workers=0, \n",
    "#     pin_memory=True\n",
    "# )\n",
    "\n",
    "# focal_loss_fn = FocalLoss(alpha=0.75, gamma=2.0).to(device)\n",
    "\n",
    "# # ===========================\n",
    "# # SUMMARY\n",
    "# # ===========================\n",
    "\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"OPTIMIZED DATALOADER SUMMARY\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"\\nüìä Training Set:\")\n",
    "# print(f\"  Total samples: {len(train_dataset):,}\")\n",
    "# print(f\"  Batches per epoch: {len(train_loader):,}\")\n",
    "# print(f\"  Batch size: {config.batch_size}\")\n",
    "# print(f\"  Sampling: WeightedRandomSampler (balanced)\")\n",
    "\n",
    "# print(f\"\\nüìä Test Set:\")\n",
    "# print(f\"  Total samples: {len(test_dataset):,}\")\n",
    "# print(f\"  Batches: {len(test_loader):,}\")\n",
    "\n",
    "# print(f\"\\nüéØ Loss Configuration:\")\n",
    "# print(f\"  Loss Function: Focal Loss\")\n",
    "# print(f\"  Alpha (Œ±): 0.75, Gamma (Œ≥): 2.0\")\n",
    "# print(f\"  pos_weight: {pos_weight.item():.4f}\")\n",
    "\n",
    "# print(f\"\\n‚úÖ GUARANTEED 1:1.33 ratio in BOTH train and test!\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8193156-d3ca-438a-92a1-4bf661c4068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AGGRESSIVE MEMORY OPTIMIZATION\n",
      "============================================================\n",
      "\n",
      "‚úÖ Memory optimization configured\n",
      "   GPU: NVIDIA RTX A6000\n",
      "   Total Memory: 48.31 GB\n",
      "============================================================\n",
      "\n",
      "Building model...\n",
      "Warning: Could not load audio model: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "Using fallback CNN encoder\n",
      "‚úÖ Model built successfully!\n",
      "Total parameters: 124,507,853\n",
      "Trainable parameters: 15,995,981\n",
      "\n",
      "Optimizer ready!\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING WITH ANTI-FREEZE PROTECTION\n",
      "============================================================\n",
      "‚ö° FIXES: Reduced batch processing, aggressive memory clearing\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EPOCH 1/10\n",
      "============================================================\n",
      "\n",
      "[TRAINING]\n",
      "  GRL Alpha: 0.0000\n",
      "  Total batches: 6,700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Training:   1%|‚ñé                       | 100/6700 [00:24<20:48,  5.29it/s, loss=0.1317, acc=56.0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Checkpoint 100/6700 - Loss: 0.1317, Acc: 56.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Training:   3%|‚ñã                       | 201/6700 [00:42<14:55,  7.25it/s, loss=0.1262, acc=60.4%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Checkpoint 200/6700 - Loss: 0.1262, Acc: 60.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Training:   4%|‚ñà                       | 301/6700 [01:01<16:31,  6.46it/s, loss=0.1209, acc=63.9%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Checkpoint 300/6700 - Loss: 0.1209, Acc: 63.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Training:   6%|‚ñà‚ñç                      | 401/6700 [01:20<35:31,  2.96it/s, loss=0.1188, acc=65.2%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Checkpoint 400/6700 - Loss: 0.1188, Acc: 65.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Training:   7%|‚ñà‚ñã                      | 467/6700 [01:29<18:40,  5.56it/s, loss=0.1179, acc=66.2%]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AGGRESSIVE MEMORY OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear all caches\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Limit PyTorch memory allocation\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.90)\n",
    "\n",
    "print(f\"\\n‚úÖ Memory optimization configured\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model\n",
    "print(\"\\nBuilding model...\")\n",
    "n_domains = 12\n",
    "model = MultimodalDeepfakeDetector(config, n_domains=n_domains).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model built successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"\\nOptimizer ready!\")\n",
    "\n",
    "# ===========================\n",
    "# TRAINING LOOP WITH ANTI-FREEZE FIXES\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING WITH ANTI-FREEZE PROTECTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚ö° FIXES: Reduced batch processing, aggressive memory clearing\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "results_history = []\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EPOCH {epoch+1}/{config.epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Clear memory before epoch\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # ==================== TRAINING ====================\n",
    "    print(\"\\n[TRAINING]\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_cls_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Update GRL alpha\n",
    "    progress = epoch / config.epochs\n",
    "    alpha = config.alpha_domain * (2 / (1 + np.exp(-10 * progress)) - 1)\n",
    "    model.set_grl_alpha(alpha)\n",
    "    print(f\"  GRL Alpha: {alpha:.4f}\")\n",
    "    \n",
    "    # ‚ö° ANTI-FREEZE FIX: Process in smaller chunks\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Calculate total batches\n",
    "    total_batches = len(train_loader)\n",
    "    print(f\"  Total batches: {total_batches:,}\")\n",
    "    \n",
    "    # Create iterator\n",
    "    train_iter = iter(train_loader)\n",
    "    \n",
    "    step = 0\n",
    "    checkpoint_interval = 100  # Save progress every 100 batches\n",
    "    \n",
    "    pbar = tqdm(total=total_batches, desc='  Training', ncols=100, leave=True)\n",
    "    \n",
    "    while step < total_batches:\n",
    "        try:\n",
    "            # ‚ö° CRITICAL: Get batch with timeout protection\n",
    "            batch = next(train_iter)\n",
    "            \n",
    "            # Move data to GPU\n",
    "            images = batch['images'].to(device, non_blocking=True) if batch['images'] is not None else None\n",
    "            audio = batch['audio'].to(device, non_blocking=True) if batch['audio'] is not None else None\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "            domains = batch['domains'].to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images=images, audio=audio, text=None, metadata=None)\n",
    "                cls_loss = focal_loss_fn(outputs['logits'].squeeze(), labels)\n",
    "                dom_loss = F.cross_entropy(outputs['domain_logits'], domains) if outputs['domain_logits'] is not None else 0\n",
    "                loss = cls_loss + alpha * dom_loss\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Track metrics\n",
    "            loss_val = loss.item() if isinstance(loss, torch.Tensor) else 0\n",
    "            cls_loss_val = cls_loss.item() if isinstance(cls_loss, torch.Tensor) else 0\n",
    "            \n",
    "            preds = (torch.sigmoid(outputs['logits']) > 0.5).float()\n",
    "            correct += (preds.squeeze() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # ‚ö° CRITICAL: Delete tensors immediately\n",
    "            del images, audio, labels, domains, outputs, loss, cls_loss, preds\n",
    "            if isinstance(dom_loss, torch.Tensor):\n",
    "                del dom_loss\n",
    "            \n",
    "            total_loss += loss_val\n",
    "            total_cls_loss += cls_loss_val\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # ‚ö° ANTI-FREEZE: Clear memory every 20 steps (not 50)\n",
    "            if step % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # ‚ö° ANTI-FREEZE: Print progress every 100 batches\n",
    "            if step % checkpoint_interval == 0:\n",
    "                current_acc = 100. * correct / total if total > 0 else 0\n",
    "                print(f\"\\n  Checkpoint {step}/{total_batches} - Loss: {total_loss/step:.4f}, Acc: {current_acc:.2f}%\")\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{total_loss/step:.4f}',\n",
    "                'acc': f'{100.*correct/total:.1f}%'\n",
    "            })\n",
    "            \n",
    "        except StopIteration:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Error at batch {step}: {e}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            continue\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    train_loss = total_loss / step if step > 0 else 0\n",
    "    train_acc = 100. * correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n  >>> TRAINING RESULTS:\")\n",
    "    print(f\"      Loss:     {train_loss:.4f}\")\n",
    "    print(f\"      Accuracy: {train_acc:.2f}%\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # ==================== EVALUATION ====================\n",
    "    print(f\"\\n[EVALUATION]\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    eval_batches = len(test_loader)\n",
    "    print(f\"  Eval batches: {eval_batches:,}\")\n",
    "    \n",
    "    pbar = tqdm(test_loader, desc='  Testing', total=eval_batches, ncols=100, leave=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            try:\n",
    "                images = batch['images'].to(device, non_blocking=True) if batch['images'] is not None else None\n",
    "                audio = batch['audio'].to(device, non_blocking=True) if batch['audio'] is not None else None\n",
    "                labels = batch['labels'].to(device, non_blocking=True)\n",
    "                \n",
    "                outputs = model(images=images, audio=audio, text=None, metadata=None, return_domain_logits=False)\n",
    "                probs = torch.sigmoid(outputs['logits'])\n",
    "                preds = (probs > 0.5).float()\n",
    "                \n",
    "                correct += (preds.squeeze() == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                \n",
    "                del images, audio, labels, outputs, probs, preds\n",
    "                \n",
    "                # Clear memory every 50 batches during eval\n",
    "                if (batch_idx + 1) % 50 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                pbar.set_postfix({'acc': f'{100.*correct/total:.1f}%'})\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è Eval error at batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_acc = 100. * correct / total if total > 0 else 0\n",
    "    test_precision = precision_score(all_labels, all_preds, zero_division=0) * 100\n",
    "    test_recall = recall_score(all_labels, all_preds, zero_division=0) * 100\n",
    "    test_f1 = f1_score(all_labels, all_preds, zero_division=0) * 100\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(f\"\\n  >>> DETAILED METRICS:\")\n",
    "    print(classification_report(all_labels, all_preds, \n",
    "                              target_names=['Real', 'Fake'], \n",
    "                              digits=2, \n",
    "                              zero_division=0))\n",
    "    \n",
    "    print(f\"\\n  >>> TEST RESULTS:\")\n",
    "    print(f\"      Accuracy:  {test_acc:.2f}%\")\n",
    "    print(f\"      Precision: {test_precision:.2f}%\")\n",
    "    print(f\"      Recall:    {test_recall:.2f}%\")\n",
    "    print(f\"      F1 Score:  {test_f1:.2f}%\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    results_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1\n",
    "    })\n",
    "    \n",
    "    if test_f1 > best_f1:\n",
    "        best_f1 = test_f1\n",
    "        best_acc = test_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'test_f1': test_f1,\n",
    "            'config': config\n",
    "        }, 'best_multimodal_balanced.pth')\n",
    "        print(f\"\\n  ‚úÖ NEW BEST! F1: {best_f1:.2f}%\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best F1: {best_f1:.2f}% | Best Acc: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fd26a-5fac-464d-8356-1253f6c4059d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DeepFake Detection)",
   "language": "python",
   "name": "deepfake_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
