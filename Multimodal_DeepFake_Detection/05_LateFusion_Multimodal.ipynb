{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846e2412",
   "metadata": {},
   "source": [
    "# 05 - Late Fusion Multimodal Deepfake Detection\n",
    "\n",
    "## Objective\n",
    "Combine predictions from individual modality classifiers (late fusion).\n",
    "\n",
    "## Approach\n",
    "1. Train separate classifiers for each modality\n",
    "2. Combine predictions using weighted voting or learned fusion\n",
    "3. Compare with early fusion\n",
    "\n",
    "## Fusion Strategies\n",
    "- Average voting\n",
    "- Weighted voting (learned weights)\n",
    "- Meta-classifier on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10a5960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akshay-stu\\miniconda3\\envs\\dl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "\n",
    "# Model imports\n",
    "import timm\n",
    "from transformers import Wav2Vec2Model\n",
    "import torchvision.models.video as video_models\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e79463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual model classes defined!\n"
     ]
    }
   ],
   "source": [
    "class ConvNeXtDeepfakeDetector(nn.Module):\n",
    "    \"\"\"Image classifier with full classification head\"\"\"\n",
    "    def __init__(self, model_name='convnext_base', freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.convnext = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.convnext.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.feature_dim = self.convnext.num_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.convnext(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "class Wav2Vec2Detector(nn.Module):\n",
    "    \"\"\"Audio classifier with full classification head\"\"\"\n",
    "    def __init__(self, model_name='facebook/wav2vec2-base', freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(model_name)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.wav2vec2.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.feature_dim = self.wav2vec2.config.hidden_size\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.wav2vec2(x)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled = self.pool(hidden_states.transpose(1, 2)).squeeze(-1)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "class ResNet3DDetector(nn.Module):\n",
    "    \"\"\"Video classifier with full classification head\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.resnet3d = video_models.r3d_18(weights='DEFAULT' if pretrained else None)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet3d.children())[:-1])\n",
    "        self.feature_dim = 512\n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.squeeze(-1).squeeze(-1).squeeze(-1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "print('Individual model classes defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6e90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set!\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "TRAIN_FAKE_DIR_IMG = '../Deepfake image detection dataset/train-20250112T065955Z-001/train/fake'\n",
    "TRAIN_REAL_DIR_IMG = '../Deepfake image detection dataset/train-20250112T065955Z-001/train/real'\n",
    "TEST_FAKE_DIR_IMG = '../Deepfake image detection dataset/test-20250112T065939Z-001/test/fake'\n",
    "TEST_REAL_DIR_IMG = '../Deepfake image detection dataset/test-20250112T065939Z-001/test/real'\n",
    "\n",
    "REAL_AUDIO_DIR = '../DeepFake_AudioDataset/KAGGLE/AUDIO/REAL'\n",
    "FAKE_AUDIO_DIR = '../DeepFake_AudioDataset/KAGGLE/AUDIO/FAKE'\n",
    "\n",
    "TRAIN_FAKE_DIR_VID = '../dfd_faces/train/fake'\n",
    "TRAIN_REAL_DIR_VID = '../dfd_faces/train/real'\n",
    "TEST_FAKE_DIR_VID = '../dfd_faces/test/fake'\n",
    "TEST_REAL_DIR_VID = '../dfd_faces/test/real'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 224\n",
    "NUM_FRAMES = 16\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = SAMPLE_RATE * 10\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "print(\"Configuration set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dfe43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late Fusion Model Defined!\n"
     ]
    }
   ],
   "source": [
    "class LateFusionDetector(nn.Module):\n",
    "    \"\"\"Late Fusion: Combines predictions from individual classifiers\"\"\"\n",
    "    def __init__(self, image_model, audio_model, video_model, fusion_type='weighted'):\n",
    "        super().__init__()\n",
    "        self.image_model = image_model\n",
    "        self.audio_model = audio_model\n",
    "        self.video_model = video_model\n",
    "        self.fusion_type = fusion_type\n",
    "        \n",
    "        if fusion_type == 'weighted':\n",
    "            # Learnable weights for each modality\n",
    "            self.weights = nn.Parameter(torch.ones(3) / 3)\n",
    "        elif fusion_type == 'meta':\n",
    "            # Meta-classifier learns to combine predictions\n",
    "            self.meta_classifier = nn.Sequential(\n",
    "                nn.Linear(6, 32),  # 3 models * 2 classes\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 2)\n",
    "            )\n",
    "    \n",
    "    def forward(self, image, audio, video):\n",
    "        # Get predictions from each modality\n",
    "        img_logits = self.image_model(image)\n",
    "        aud_logits = self.audio_model(audio)\n",
    "        vid_logits = self.video_model(video)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        img_pred = torch.softmax(img_logits, dim=1)\n",
    "        aud_pred = torch.softmax(aud_logits, dim=1)\n",
    "        vid_pred = torch.softmax(vid_logits, dim=1)\n",
    "        \n",
    "        if self.fusion_type == 'average':\n",
    "            # Simple average voting\n",
    "            output = (img_pred + aud_pred + vid_pred) / 3\n",
    "        elif self.fusion_type == 'weighted':\n",
    "            # Weighted voting with learnable weights\n",
    "            weights = torch.softmax(self.weights, dim=0)\n",
    "            output = weights[0] * img_pred + weights[1] * aud_pred + weights[2] * vid_pred\n",
    "        elif self.fusion_type == 'meta':\n",
    "            # Meta-classifier on concatenated predictions\n",
    "            combined = torch.cat([img_pred, aud_pred, vid_pred], dim=1)\n",
    "            output = torch.softmax(self.meta_classifier(combined), dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print('Late Fusion Model Defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf6e87",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Late Fusion Strategies Tested:\n",
    "1. **Average Fusion**: Simple average of predictions from all modalities\n",
    "2. **Weighted Fusion**: Learnable weights for each modality (optimized during training)\n",
    "3. **Meta-Classifier**: Neural network learns to combine predictions\n",
    "\n",
    "### Key Insights:\n",
    "- Late fusion allows each modality to make independent predictions\n",
    "- Individual models are frozen (pretrained from baseline notebooks)\n",
    "- Only fusion mechanism is trained\n",
    "- Faster training than early fusion (fewer parameters to update)\n",
    "\n",
    "### Comparison with Early Fusion:\n",
    "- Early fusion: Combines features before classification\n",
    "- Late fusion: Combines predictions after classification\n",
    "- Trade-off: Early fusion learns joint representations, late fusion preserves modality-specific decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a84531",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize best fusion strategy in detail\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_fusion \u001b[38;5;241m=\u001b[39m \u001b[43mcomparison_df\u001b[49m\u001b[38;5;241m.\u001b[39mloc[comparison_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmax(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfusion_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m best_data \u001b[38;5;241m=\u001b[39m results[best_fusion]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDetailed Analysis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_fusion\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Fusion (Best Performer)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize best fusion strategy in detail\n",
    "best_fusion = comparison_df.loc[comparison_df['accuracy'].idxmax(), 'fusion_type']\n",
    "best_data = results[best_fusion]\n",
    "\n",
    "print(f\"\\nDetailed Analysis: {best_fusion.upper()} Fusion (Best Performer)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(best_data['train_losses'], label='Train', marker='o')\n",
    "axes[0, 0].plot(best_data['val_losses'], label='Val', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title(f'{best_fusion.upper()} - Loss Curves')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0, 1].plot(best_data['train_accs'], label='Train', marker='o')\n",
    "axes[0, 1].plot(best_data['val_accs'], label='Val', marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title(f'{best_fusion.upper()} - Accuracy Curves')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(best_data['final_labels'], best_data['final_preds'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=['Real', 'Fake'],\n",
    "            yticklabels=['Real', 'Fake'])\n",
    "axes[1, 0].set_title(f'{best_fusion.upper()} - Confusion Matrix')\n",
    "axes[1, 0].set_ylabel('True Label')\n",
    "axes[1, 0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "if len(np.unique(best_data['final_labels'])) > 1:\n",
    "    fpr, tpr, _ = roc_curve(best_data['final_labels'], best_data['final_probs'])\n",
    "    roc_auc = roc_auc_score(best_data['final_labels'], best_data['final_probs'])\n",
    "    \n",
    "    axes[1, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    axes[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axes[1, 1].set_xlabel('False Positive Rate')\n",
    "    axes[1, 1].set_ylabel('True Positive Rate')\n",
    "    axes[1, 1].set_title(f'{best_fusion.upper()} - ROC Curve')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'late_fusion_{best_fusion}_detailed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Detailed plot saved as 'late_fusion_{best_fusion}_detailed.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a183a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Accuracy comparison\u001b[39;00m\n\u001b[0;32m      5\u001b[0m ax \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m ax\u001b[38;5;241m.\u001b[39mbar(\u001b[43mcomparison_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfusion_type\u001b[39m\u001b[38;5;124m'\u001b[39m], comparison_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy Comparison\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax = axes[0, 0]\n",
    "ax.bar(comparison_df['fusion_type'], comparison_df['accuracy'])\n",
    "ax.set_title('Accuracy Comparison')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(comparison_df['fusion_type'], comparison_df['f1'])\n",
    "ax.set_title('F1-Score Comparison')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# ROC-AUC comparison\n",
    "ax = axes[1, 0]\n",
    "ax.bar(comparison_df['fusion_type'], comparison_df['roc_auc'])\n",
    "ax.set_title('ROC-AUC Comparison')\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "ax = axes[1, 1]\n",
    "ax.bar(comparison_df['fusion_type'], comparison_df['training_time'])\n",
    "ax.set_title('Training Time Comparison')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('late_fusion_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comparison plot saved as 'late_fusion_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44999a7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compare all fusion strategies\u001b[39;00m\n\u001b[0;32m      2\u001b[0m comparison_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fusion_type, data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      4\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m     comparison_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfusion_type\u001b[39m\u001b[38;5;124m'\u001b[39m: fusion_type,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m     })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare all fusion strategies\n",
    "comparison_data = []\n",
    "for fusion_type, data in results.items():\n",
    "    metrics = data['metrics']\n",
    "    comparison_data.append({\n",
    "        'fusion_type': fusion_type,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1': metrics['f1'],\n",
    "        'roc_auc': metrics['roc_auc'],\n",
    "        'training_time': metrics['training_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nLate Fusion Strategy Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('late_fusion_results.csv', index=False)\n",
    "print(\"\\n✓ Results saved as 'late_fusion_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e156c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Late Fusion: AVERAGE\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m LateFusionDetector(\u001b[43mimage_model\u001b[49m, audio_model, video_model, fusion_type\u001b[38;5;241m=\u001b[39mfusion_type)\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Show trainable parameters\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Train all three fusion strategies\n",
    "fusion_types = ['average', 'weighted', 'meta']\n",
    "results = {}\n",
    "\n",
    "for fusion_type in fusion_types:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Late Fusion: {fusion_type.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LateFusionDetector(image_model, audio_model, video_model, fusion_type=fusion_type)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Show trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Setup training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                           lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_preds, val_labels, val_probs = evaluate(model, test_loader, criterion, device)\n",
    "        val_acc = accuracy_score(val_labels, val_preds) * 100\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = model.state_dict().copy()\n",
    "            print(f\"✓ New best accuracy: {best_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(f\"\\nFinal evaluation on best model...\")\n",
    "    _, final_preds, final_labels, final_probs = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    metrics = calculate_metrics(final_labels, final_preds, final_probs)\n",
    "    metrics['training_time'] = training_time\n",
    "    metrics['best_val_acc'] = best_acc\n",
    "    metrics['fusion_type'] = fusion_type\n",
    "    \n",
    "    # Store results\n",
    "    results[fusion_type] = {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'final_preds': final_preds,\n",
    "        'final_labels': final_labels,\n",
    "        'final_probs': final_probs\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{fusion_type.upper()} Fusion Results:\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, value in metrics.items():\n",
    "        if key == 'training_time':\n",
    "            print(f\"{key}: {value:.2f}s ({value/60:.2f} min)\")\n",
    "        elif key != 'fusion_type':\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'late_fusion_{fusion_type}_model.pth')\n",
    "    print(f\"✓ Model saved as 'late_fusion_{fusion_type}_model.pth'\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL LATE FUSION TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b614eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for image, audio, video, labels in pbar:\n",
    "        image = image.to(device)\n",
    "        audio = audio.to(device)\n",
    "        video = video.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, audio, video)\n",
    "        \n",
    "        # For late fusion, output is already probabilities\n",
    "        # Convert back to logits for CrossEntropyLoss\n",
    "        outputs = torch.log(outputs + 1e-8)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{running_loss/len(pbar):.4f}', \n",
    "                         'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Evaluating')\n",
    "        for image, audio, video, labels in pbar:\n",
    "            image = image.to(device)\n",
    "            audio = audio.to(device)\n",
    "            video = video.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(image, audio, video)\n",
    "            outputs_log = torch.log(outputs + 1e-8)\n",
    "            loss = criterion(outputs_log, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs[:, 1].cpu().numpy())\n",
    "    \n",
    "    return running_loss / len(loader), all_preds, all_labels, all_probs\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_probs):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_probs) if len(np.unique(y_true)) > 1 else 0.0\n",
    "    }\n",
    "\n",
    "print('Training functions defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pretrained models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load image model\n",
    "image_model = ConvNeXtDeepfakeDetector(freeze_backbone=True)\n",
    "if os.path.exists('best_image_model_convnext.pth'):\n",
    "    print(\"✓ Loading saved ConvNeXt weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_image_model_convnext.pth', map_location=device)\n",
    "        image_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  ConvNeXt loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved ConvNeXt found, using pretrained from timm\")\n",
    "\n",
    "# Load audio model\n",
    "audio_model = Wav2Vec2Detector(freeze_backbone=True)\n",
    "if os.path.exists('best_audio_model_wav2vec2.pth'):\n",
    "    print(\"✓ Loading saved Wav2Vec2 weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_audio_model_wav2vec2.pth', map_location=device)\n",
    "        audio_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  Wav2Vec2 loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved Wav2Vec2 found, using pretrained from HuggingFace\")\n",
    "\n",
    "# Load video model\n",
    "video_model = ResNet3DDetector(pretrained=True)\n",
    "if os.path.exists('best_video_model_resnet3d.pth'):\n",
    "    print(\"✓ Loading saved 3D ResNet weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_video_model_resnet3d.pth', map_location=device)\n",
    "        video_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  3D ResNet loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved 3D ResNet found, using pretrained from torchvision\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Freeze all individual model parameters\n",
    "for param in image_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in audio_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in video_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\nIndividual models loaded and frozen!\")\n",
    "print(\"Ready for late fusion training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13241ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultimodalDeepfakeDataset(mode='train')\n",
    "test_dataset = MultimodalDeepfakeDataset(mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21806f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDeepfakeDataset(Dataset):\n",
    "    \"\"\"Dataset that returns image, audio, and video for the same sample\"\"\"\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.img_fake_dir = TRAIN_FAKE_DIR_IMG\n",
    "            self.img_real_dir = TRAIN_REAL_DIR_IMG\n",
    "            self.vid_fake_dir = TRAIN_FAKE_DIR_VID\n",
    "            self.vid_real_dir = TRAIN_REAL_DIR_VID\n",
    "        else:\n",
    "            self.img_fake_dir = TEST_FAKE_DIR_IMG\n",
    "            self.img_real_dir = TEST_REAL_DIR_IMG\n",
    "            self.vid_fake_dir = TEST_FAKE_DIR_VID\n",
    "            self.vid_real_dir = TEST_REAL_DIR_VID\n",
    "        \n",
    "        # Load image paths\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        if os.path.exists(self.img_fake_dir):\n",
    "            fake_imgs = [os.path.join(self.img_fake_dir, f) for f in os.listdir(self.img_fake_dir) \n",
    "                        if f.endswith(('.jpg', '.png'))]\n",
    "            self.image_paths.extend(fake_imgs[:100])\n",
    "            self.labels.extend([1] * len(fake_imgs[:100]))\n",
    "        \n",
    "        if os.path.exists(self.img_real_dir):\n",
    "            real_imgs = [os.path.join(self.img_real_dir, f) for f in os.listdir(self.img_real_dir) \n",
    "                        if f.endswith(('.jpg', '.png'))]\n",
    "            self.image_paths.extend(real_imgs[:100])\n",
    "            self.labels.extend([0] * len(real_imgs[:100]))\n",
    "        \n",
    "        # Load audio paths\n",
    "        self.audio_paths = []\n",
    "        if os.path.exists(FAKE_AUDIO_DIR):\n",
    "            fake_aud = [os.path.join(FAKE_AUDIO_DIR, f) for f in os.listdir(FAKE_AUDIO_DIR) \n",
    "                       if f.endswith(('.wav', '.mp3'))][:50]\n",
    "            self.audio_paths.extend(fake_aud)\n",
    "        \n",
    "        if os.path.exists(REAL_AUDIO_DIR):\n",
    "            real_aud = [os.path.join(REAL_AUDIO_DIR, f) for f in os.listdir(REAL_AUDIO_DIR) \n",
    "                       if f.endswith(('.wav', '.mp3'))][:50]\n",
    "            self.audio_paths.extend(real_aud)\n",
    "        \n",
    "        # Load video folders\n",
    "        self.video_folders = []\n",
    "        if os.path.exists(self.vid_fake_dir):\n",
    "            fake_vids = [os.path.join(self.vid_fake_dir, f) for f in os.listdir(self.vid_fake_dir) \n",
    "                        if os.path.isdir(os.path.join(self.vid_fake_dir, f))][:50]\n",
    "            self.video_folders.extend(fake_vids)\n",
    "        \n",
    "        if os.path.exists(self.vid_real_dir):\n",
    "            real_vids = [os.path.join(self.vid_real_dir, f) for f in os.listdir(self.vid_real_dir) \n",
    "                        if os.path.isdir(os.path.join(self.vid_real_dir, f))][:50]\n",
    "            self.video_folders.extend(real_vids)\n",
    "        \n",
    "        # Image transforms\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(f\"{mode.upper()}: {len(self.image_paths)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.image_paths), len(self.audio_paths), len(self.video_folders))\n",
    "    \n",
    "    def load_image(self, idx):\n",
    "        img_path = self.image_paths[idx % len(self.image_paths)]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            return self.img_transform(img)\n",
    "        except:\n",
    "            return torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "    \n",
    "    def load_audio(self, idx):\n",
    "        aud_path = self.audio_paths[idx % len(self.audio_paths)]\n",
    "        try:\n",
    "            audio, _ = librosa.load(aud_path, sr=SAMPLE_RATE, duration=10)\n",
    "            if len(audio) < MAX_AUDIO_LENGTH:\n",
    "                audio = np.pad(audio, (0, MAX_AUDIO_LENGTH - len(audio)))\n",
    "            else:\n",
    "                audio = audio[:MAX_AUDIO_LENGTH]\n",
    "            return torch.FloatTensor(audio)\n",
    "        except:\n",
    "            return torch.zeros(MAX_AUDIO_LENGTH)\n",
    "    \n",
    "    def load_video(self, idx):\n",
    "        vid_folder = self.video_folders[idx % len(self.video_folders)]\n",
    "        try:\n",
    "            frames_files = sorted([f for f in os.listdir(vid_folder) if f.endswith(('.jpg', '.png'))])\n",
    "            indices = np.linspace(0, len(frames_files)-1, NUM_FRAMES, dtype=int)\n",
    "            \n",
    "            frames = []\n",
    "            for i in indices:\n",
    "                img = Image.open(os.path.join(vid_folder, frames_files[i])).convert('RGB')\n",
    "                frames.append(self.img_transform(img))\n",
    "            \n",
    "            return torch.stack(frames)\n",
    "        except:\n",
    "            return torch.zeros(NUM_FRAMES, 3, IMG_SIZE, IMG_SIZE)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.load_image(idx)\n",
    "        audio = self.load_audio(idx)\n",
    "        video = self.load_video(idx)\n",
    "        label = self.labels[idx % len(self.labels)]\n",
    "        \n",
    "        return image, audio, video, label\n",
    "\n",
    "print('Multimodal Dataset defined!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
