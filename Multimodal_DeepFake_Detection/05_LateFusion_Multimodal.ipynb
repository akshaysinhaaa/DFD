{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846e2412",
   "metadata": {},
   "source": [
    "# 05 - Late Fusion Multimodal Deepfake Detection\n",
    "\n",
    "## Objective\n",
    "Combine predictions from individual modality classifiers (late fusion).\n",
    "\n",
    "## Approach\n",
    "1. Train separate classifiers for each modality\n",
    "2. Combine predictions using weighted voting or learned fusion\n",
    "3. Compare with early fusion\n",
    "\n",
    "## Fusion Strategies\n",
    "- Average voting\n",
    "- Weighted voting (learned weights)\n",
    "- Meta-classifier on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b10a5960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "\n",
    "# Model imports\n",
    "import timm\n",
    "from transformers import Wav2Vec2Model\n",
    "import torchvision.models.video as video_models\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63e79463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual model classes defined!\n"
     ]
    }
   ],
   "source": [
    "class ConvNeXtDeepfakeDetector(nn.Module):\n",
    "    \"\"\"Image classifier with full classification head\"\"\"\n",
    "    def __init__(self, model_name='convnext_base', freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.convnext = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.convnext.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.feature_dim = self.convnext.num_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.convnext(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "class Wav2Vec2Detector(nn.Module):\n",
    "    \"\"\"Audio classifier with full classification head\"\"\"\n",
    "    def __init__(self, model_name='facebook/wav2vec2-base', freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(model_name)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.wav2vec2.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.feature_dim = self.wav2vec2.config.hidden_size\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.wav2vec2(x)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled = self.pool(hidden_states.transpose(1, 2)).squeeze(-1)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "class ResNet3DDetector(nn.Module):\n",
    "    \"\"\"Video classifier with full classification head\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.resnet3d = video_models.r3d_18(weights='DEFAULT' if pretrained else None)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet3d.children())[:-1])\n",
    "        self.feature_dim = 512\n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.squeeze(-1).squeeze(-1).squeeze(-1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "print('Individual model classes defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e6e90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set!\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "TRAIN_FAKE_DIR_IMG = '../Deepfake image detection dataset/train-20250112T065955Z-001/train/fake'\n",
    "TRAIN_REAL_DIR_IMG = '../Deepfake image detection dataset/train-20250112T065955Z-001/train/real'\n",
    "TEST_FAKE_DIR_IMG = '../Deepfake image detection dataset/test-20250112T065939Z-001/test/fake'\n",
    "TEST_REAL_DIR_IMG = '../Deepfake image detection dataset/test-20250112T065939Z-001/test/real'\n",
    "\n",
    "REAL_AUDIO_DIR = '../DeepFake_AudioDataset/KAGGLE/AUDIO/REAL'\n",
    "FAKE_AUDIO_DIR = '../DeepFake_AudioDataset/KAGGLE/AUDIO/FAKE'\n",
    "\n",
    "TRAIN_FAKE_DIR_VID = '../dfd_faces/train/fake'\n",
    "TRAIN_REAL_DIR_VID = '../dfd_faces/train/real'\n",
    "TEST_FAKE_DIR_VID = '../dfd_faces/test/fake'\n",
    "TEST_REAL_DIR_VID = '../dfd_faces/test/real'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 224\n",
    "NUM_FRAMES = 16\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = SAMPLE_RATE * 10\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "print(\"Configuration set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "058603be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal Dataset defined!\n"
     ]
    }
   ],
   "source": [
    "class MultimodalDeepfakeDataset(Dataset):\n",
    "    \"\"\"Dataset that returns image, audio, and video for the same sample\"\"\"\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.img_fake_dir = TRAIN_FAKE_DIR_IMG\n",
    "            self.img_real_dir = TRAIN_REAL_DIR_IMG\n",
    "            self.vid_fake_dir = TRAIN_FAKE_DIR_VID\n",
    "            self.vid_real_dir = TRAIN_REAL_DIR_VID\n",
    "        else:\n",
    "            self.img_fake_dir = TEST_FAKE_DIR_IMG\n",
    "            self.img_real_dir = TEST_REAL_DIR_IMG\n",
    "            self.vid_fake_dir = TEST_FAKE_DIR_VID\n",
    "            self.vid_real_dir = TEST_REAL_DIR_VID\n",
    "        \n",
    "        # Load image paths\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        if os.path.exists(self.img_fake_dir):\n",
    "            fake_imgs = [os.path.join(self.img_fake_dir, f) for f in os.listdir(self.img_fake_dir) \n",
    "                        if f.endswith(('.jpg', '.png'))]\n",
    "            self.image_paths.extend(fake_imgs[:100])\n",
    "            self.labels.extend([1] * len(fake_imgs[:100]))\n",
    "        \n",
    "        if os.path.exists(self.img_real_dir):\n",
    "            real_imgs = [os.path.join(self.img_real_dir, f) for f in os.listdir(self.img_real_dir) \n",
    "                        if f.endswith(('.jpg', '.png'))]\n",
    "            self.image_paths.extend(real_imgs[:100])\n",
    "            self.labels.extend([0] * len(real_imgs[:100]))\n",
    "        \n",
    "        # Load audio paths\n",
    "        self.audio_paths = []\n",
    "        if os.path.exists(FAKE_AUDIO_DIR):\n",
    "            fake_aud = [os.path.join(FAKE_AUDIO_DIR, f) for f in os.listdir(FAKE_AUDIO_DIR) \n",
    "                       if f.endswith(('.wav', '.mp3'))][:50]\n",
    "            self.audio_paths.extend(fake_aud)\n",
    "        \n",
    "        if os.path.exists(REAL_AUDIO_DIR):\n",
    "            real_aud = [os.path.join(REAL_AUDIO_DIR, f) for f in os.listdir(REAL_AUDIO_DIR) \n",
    "                       if f.endswith(('.wav', '.mp3'))][:50]\n",
    "            self.audio_paths.extend(real_aud)\n",
    "        \n",
    "        # Load video folders\n",
    "        self.video_folders = []\n",
    "        if os.path.exists(self.vid_fake_dir):\n",
    "            fake_vids = [os.path.join(self.vid_fake_dir, f) for f in os.listdir(self.vid_fake_dir) \n",
    "                        if os.path.isdir(os.path.join(self.vid_fake_dir, f))][:50]\n",
    "            self.video_folders.extend(fake_vids)\n",
    "        \n",
    "        if os.path.exists(self.vid_real_dir):\n",
    "            real_vids = [os.path.join(self.vid_real_dir, f) for f in os.listdir(self.vid_real_dir) \n",
    "                        if os.path.isdir(os.path.join(self.vid_real_dir, f))][:50]\n",
    "            self.video_folders.extend(real_vids)\n",
    "        \n",
    "        # Image transforms\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(f\"{mode.upper()}: {len(self.image_paths)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(len(self.image_paths), len(self.audio_paths), len(self.video_folders))\n",
    "    \n",
    "    def load_image(self, idx):\n",
    "        img_path = self.image_paths[idx % len(self.image_paths)]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            return self.img_transform(img)\n",
    "        except:\n",
    "            return torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "    \n",
    "    def load_audio(self, idx):\n",
    "        aud_path = self.audio_paths[idx % len(self.audio_paths)]\n",
    "        try:\n",
    "            audio, _ = librosa.load(aud_path, sr=SAMPLE_RATE, duration=10)\n",
    "            if len(audio) < MAX_AUDIO_LENGTH:\n",
    "                audio = np.pad(audio, (0, MAX_AUDIO_LENGTH - len(audio)))\n",
    "            else:\n",
    "                audio = audio[:MAX_AUDIO_LENGTH]\n",
    "            return torch.FloatTensor(audio)\n",
    "        except:\n",
    "            return torch.zeros(MAX_AUDIO_LENGTH)\n",
    "    \n",
    "    def load_video(self, idx):\n",
    "        vid_folder = self.video_folders[idx % len(self.video_folders)]\n",
    "        try:\n",
    "            frames_files = sorted([f for f in os.listdir(vid_folder) if f.endswith(('.jpg', '.png'))])\n",
    "            indices = np.linspace(0, len(frames_files)-1, NUM_FRAMES, dtype=int)\n",
    "            \n",
    "            frames = []\n",
    "            for i in indices:\n",
    "                img = Image.open(os.path.join(vid_folder, frames_files[i])).convert('RGB')\n",
    "                frames.append(self.img_transform(img))\n",
    "            \n",
    "            return torch.stack(frames)\n",
    "        except:\n",
    "            return torch.zeros(NUM_FRAMES, 3, IMG_SIZE, IMG_SIZE)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.load_image(idx)\n",
    "        audio = self.load_audio(idx)\n",
    "        video = self.load_video(idx)\n",
    "        label = self.labels[idx % len(self.labels)]\n",
    "        \n",
    "        return image, audio, video, label\n",
    "\n",
    "print('Multimodal Dataset defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1dfe43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Late Fusion Model Defined!\n"
     ]
    }
   ],
   "source": [
    "class LateFusionDetector(nn.Module):\n",
    "    \"\"\"Late Fusion: Combines predictions from individual classifiers\"\"\"\n",
    "    def __init__(self, image_model, audio_model, video_model, fusion_type='weighted'):\n",
    "        super().__init__()\n",
    "        self.image_model = image_model\n",
    "        self.audio_model = audio_model\n",
    "        self.video_model = video_model\n",
    "        self.fusion_type = fusion_type\n",
    "        \n",
    "        if fusion_type == 'weighted':\n",
    "            # Learnable weights for each modality\n",
    "            self.weights = nn.Parameter(torch.ones(3) / 3)\n",
    "        elif fusion_type == 'meta':\n",
    "            # Meta-classifier learns to combine predictions\n",
    "            self.meta_classifier = nn.Sequential(\n",
    "                nn.Linear(6, 32),  # 3 models * 2 classes\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 2)\n",
    "            )\n",
    "    \n",
    "    def forward(self, image, audio, video):\n",
    "        # Get predictions from each modality\n",
    "        img_logits = self.image_model(image)\n",
    "        aud_logits = self.audio_model(audio)\n",
    "        vid_logits = self.video_model(video)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        img_pred = torch.softmax(img_logits, dim=1)\n",
    "        aud_pred = torch.softmax(aud_logits, dim=1)\n",
    "        vid_pred = torch.softmax(vid_logits, dim=1)\n",
    "        \n",
    "        if self.fusion_type == 'average':\n",
    "            # Simple average voting\n",
    "            output = (img_pred + aud_pred + vid_pred) / 3\n",
    "        elif self.fusion_type == 'weighted':\n",
    "            # Weighted voting with learnable weights\n",
    "            weights = torch.softmax(self.weights, dim=0)\n",
    "            output = weights[0] * img_pred + weights[1] * aud_pred + weights[2] * vid_pred\n",
    "        elif self.fusion_type == 'meta':\n",
    "            # Meta-classifier on concatenated predictions\n",
    "            combined = torch.cat([img_pred, aud_pred, vid_pred], dim=1)\n",
    "            output = torch.softmax(self.meta_classifier(combined), dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print('Late Fusion Model Defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e2981",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Late Fusion Strategies Tested:\n",
    "1. **Average Fusion**: Simple average of predictions from all modalities\n",
    "2. **Weighted Fusion**: Learnable weights for each modality (optimized during training)\n",
    "3. **Meta-Classifier**: Neural network learns to combine predictions\n",
    "\n",
    "### Key Insights:\n",
    "- Late fusion allows each modality to make independent predictions\n",
    "- Individual models are frozen (pretrained from baseline notebooks)\n",
    "- Only fusion mechanism is trained\n",
    "- Faster training than early fusion (fewer parameters to update)\n",
    "\n",
    "### Comparison with Early Fusion:\n",
    "- Early fusion: Combines features before classification\n",
    "- Late fusion: Combines predictions after classification\n",
    "- Trade-off: Early fusion learns joint representations, late fusion preserves modality-specific decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8d07a2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize best fusion strategy in detail\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_fusion \u001b[38;5;241m=\u001b[39m comparison_df\u001b[38;5;241m.\u001b[39mloc[\u001b[43mcomparison_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39midxmax(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfusion_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m best_data \u001b[38;5;241m=\u001b[39m results[best_fusion]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDetailed Analysis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_fusion\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Fusion (Best Performer)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dl\\lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dl\\lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "# Visualize best fusion strategy in detail\n",
    "best_fusion = comparison_df.loc[comparison_df['accuracy'].idxmax(), 'fusion_type']\n",
    "best_data = results[best_fusion]\n",
    "\n",
    "print(f\"\\nDetailed Analysis: {best_fusion.upper()} Fusion (Best Performer)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(best_data['train_losses'], label='Train', marker='o')\n",
    "axes[0, 0].plot(best_data['val_losses'], label='Val', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title(f'{best_fusion.upper()} - Loss Curves')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0, 1].plot(best_data['train_accs'], label='Train', marker='o')\n",
    "axes[0, 1].plot(best_data['val_accs'], label='Val', marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title(f'{best_fusion.upper()} - Accuracy Curves')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(best_data['final_labels'], best_data['final_preds'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=['Real', 'Fake'],\n",
    "            yticklabels=['Real', 'Fake'])\n",
    "axes[1, 0].set_title(f'{best_fusion.upper()} - Confusion Matrix')\n",
    "axes[1, 0].set_ylabel('True Label')\n",
    "axes[1, 0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "if len(np.unique(best_data['final_labels'])) > 1:\n",
    "    fpr, tpr, _ = roc_curve(best_data['final_labels'], best_data['final_probs'])\n",
    "    roc_auc = roc_auc_score(best_data['final_labels'], best_data['final_probs'])\n",
    "    \n",
    "    axes[1, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    axes[1, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    axes[1, 1].set_xlabel('False Positive Rate')\n",
    "    axes[1, 1].set_ylabel('True Positive Rate')\n",
    "    axes[1, 1].set_title(f'{best_fusion.upper()} - ROC Curve')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'late_fusion_{best_fusion}_detailed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Detailed plot saved as 'late_fusion_{best_fusion}_detailed.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax = axes[0, 0]\n",
    "ax.bar(comparison_df['fusion_type'], comparison_df['accuracy'])\n",
    "ax.set_title('Accuracy Comparison')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(comparison_df['fusion_type'], comparison_df['f1'])\n",
    "ax.set_title('F1-Score Comparison')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# ROC-AUC comparison\n",
    "ax = axes[1, 0]\n",
    "ax.bar(comparison_df['fusion_type'], comparison_df['roc_auc'])\n",
    "ax.set_title('ROC-AUC Comparison')\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "ax = axes[1, 1]\n",
    "ax.bar(comparison_df['fusion_type'], comparison_df['training_time'])\n",
    "ax.set_title('Training Time Comparison')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('late_fusion_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comparison plot saved as 'late_fusion_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e720fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all fusion strategies\n",
    "comparison_data = []\n",
    "for fusion_type, data in results.items():\n",
    "    metrics = data['metrics']\n",
    "    comparison_data.append({\n",
    "        'fusion_type': fusion_type,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1': metrics['f1'],\n",
    "        'roc_auc': metrics['roc_auc'],\n",
    "        'training_time': metrics['training_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nLate Fusion Strategy Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('late_fusion_results.csv', index=False)\n",
    "print(\"\\n✓ Results saved as 'late_fusion_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da2769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all three fusion strategies\n",
    "fusion_types = ['average', 'weighted', 'meta']\n",
    "results = {}\n",
    "\n",
    "for fusion_type in fusion_types:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Late Fusion: {fusion_type.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LateFusionDetector(image_model, audio_model, video_model, fusion_type=fusion_type)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Show trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Setup training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                           lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_preds, val_labels, val_probs = evaluate(model, test_loader, criterion, device)\n",
    "        val_acc = accuracy_score(val_labels, val_preds) * 100\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = model.state_dict().copy()\n",
    "            print(f\"✓ New best accuracy: {best_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(f\"\\nFinal evaluation on best model...\")\n",
    "    _, final_preds, final_labels, final_probs = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    metrics = calculate_metrics(final_labels, final_preds, final_probs)\n",
    "    metrics['training_time'] = training_time\n",
    "    metrics['best_val_acc'] = best_acc\n",
    "    metrics['fusion_type'] = fusion_type\n",
    "    \n",
    "    # Store results\n",
    "    results[fusion_type] = {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'final_preds': final_preds,\n",
    "        'final_labels': final_labels,\n",
    "        'final_probs': final_probs\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{fusion_type.upper()} Fusion Results:\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, value in metrics.items():\n",
    "        if key == 'training_time':\n",
    "            print(f\"{key}: {value:.2f}s ({value/60:.2f} min)\")\n",
    "        elif key != 'fusion_type':\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'late_fusion_{fusion_type}_model.pth')\n",
    "    print(f\"✓ Model saved as 'late_fusion_{fusion_type}_model.pth'\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL LATE FUSION TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe90c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for image, audio, video, labels in pbar:\n",
    "        image = image.to(device)\n",
    "        audio = audio.to(device)\n",
    "        video = video.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, audio, video)\n",
    "        \n",
    "        # For late fusion, output is already probabilities\n",
    "        # Convert back to logits for CrossEntropyLoss\n",
    "        outputs = torch.log(outputs + 1e-8)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{running_loss/len(pbar):.4f}', \n",
    "                         'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Evaluating')\n",
    "        for image, audio, video, labels in pbar:\n",
    "            image = image.to(device)\n",
    "            audio = audio.to(device)\n",
    "            video = video.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(image, audio, video)\n",
    "            outputs_log = torch.log(outputs + 1e-8)\n",
    "            loss = criterion(outputs_log, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs[:, 1].cpu().numpy())\n",
    "    \n",
    "    return running_loss / len(loader), all_preds, all_labels, all_probs\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_probs):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_probs) if len(np.unique(y_true)) > 1 else 0.0\n",
    "    }\n",
    "\n",
    "print('Training functions defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d43cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pretrained models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load image model\n",
    "image_model = ConvNeXtDeepfakeDetector(freeze_backbone=True)\n",
    "if os.path.exists('best_image_model_convnext.pth'):\n",
    "    print(\"✓ Loading saved ConvNeXt weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_image_model_convnext.pth', map_location=device)\n",
    "        image_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  ConvNeXt loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved ConvNeXt found, using pretrained from timm\")\n",
    "\n",
    "# Load audio model\n",
    "audio_model = Wav2Vec2Detector(freeze_backbone=True)\n",
    "if os.path.exists('best_audio_model_wav2vec2.pth'):\n",
    "    print(\"✓ Loading saved Wav2Vec2 weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_audio_model_wav2vec2.pth', map_location=device)\n",
    "        audio_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  Wav2Vec2 loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved Wav2Vec2 found, using pretrained from HuggingFace\")\n",
    "\n",
    "# Load video model\n",
    "video_model = ResNet3DDetector(pretrained=True)\n",
    "if os.path.exists('best_video_model_resnet3d.pth'):\n",
    "    print(\"✓ Loading saved 3D ResNet weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_video_model_resnet3d.pth', map_location=device)\n",
    "        video_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  3D ResNet loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved 3D ResNet found, using pretrained from torchvision\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Freeze all individual model parameters\n",
    "for param in image_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in audio_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in video_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\nIndividual models loaded and frozen!\")\n",
    "print(\"Ready for late fusion training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultimodalDeepfakeDataset(mode='train')\n",
    "test_dataset = MultimodalDeepfakeDataset(mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e156c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Late Fusion: AVERAGE\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m LateFusionDetector(\u001b[43mimage_model\u001b[49m, audio_model, video_model, fusion_type\u001b[38;5;241m=\u001b[39mfusion_type)\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Show trainable parameters\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Train all three fusion strategies\n",
    "fusion_types = ['average', 'weighted', 'meta']\n",
    "results = {}\n",
    "\n",
    "for fusion_type in fusion_types:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Late Fusion: {fusion_type.upper()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LateFusionDetector(image_model, audio_model, video_model, fusion_type=fusion_type)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Show trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Setup training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                           lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    best_acc = 0.0\n",
    "    best_state = None\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_preds, val_labels, val_probs = evaluate(model, test_loader, criterion, device)\n",
    "        val_acc = accuracy_score(val_labels, val_preds) * 100\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = model.state_dict().copy()\n",
    "            print(f\"✓ New best accuracy: {best_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(f\"\\nFinal evaluation on best model...\")\n",
    "    _, final_preds, final_labels, final_probs = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    metrics = calculate_metrics(final_labels, final_preds, final_probs)\n",
    "    metrics['training_time'] = training_time\n",
    "    metrics['best_val_acc'] = best_acc\n",
    "    metrics['fusion_type'] = fusion_type\n",
    "    \n",
    "    # Store results\n",
    "    results[fusion_type] = {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'final_preds': final_preds,\n",
    "        'final_labels': final_labels,\n",
    "        'final_probs': final_probs\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{fusion_type.upper()} Fusion Results:\")\n",
    "    print(\"-\" * 60)\n",
    "    for key, value in metrics.items():\n",
    "        if key == 'training_time':\n",
    "            print(f\"{key}: {value:.2f}s ({value/60:.2f} min)\")\n",
    "        elif key != 'fusion_type':\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'late_fusion_{fusion_type}_model.pth')\n",
    "    print(f\"✓ Model saved as 'late_fusion_{fusion_type}_model.pth'\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL LATE FUSION TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b614eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for image, audio, video, labels in pbar:\n",
    "        image = image.to(device)\n",
    "        audio = audio.to(device)\n",
    "        video = video.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, audio, video)\n",
    "        \n",
    "        # For late fusion, output is already probabilities\n",
    "        # Convert back to logits for CrossEntropyLoss\n",
    "        outputs = torch.log(outputs + 1e-8)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{running_loss/len(pbar):.4f}', \n",
    "                         'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Evaluating')\n",
    "        for image, audio, video, labels in pbar:\n",
    "            image = image.to(device)\n",
    "            audio = audio.to(device)\n",
    "            video = video.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(image, audio, video)\n",
    "            outputs_log = torch.log(outputs + 1e-8)\n",
    "            loss = criterion(outputs_log, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs[:, 1].cpu().numpy())\n",
    "    \n",
    "    return running_loss / len(loader), all_preds, all_labels, all_probs\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_probs):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_probs) if len(np.unique(y_true)) > 1 else 0.0\n",
    "    }\n",
    "\n",
    "print('Training functions defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2599cc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained models...\n",
      "============================================================\n",
      "✓ Loading saved ConvNeXt weights\n",
      "  Warning: Could not load weights (Error(s) in loading state_dict for ConvNeXtDeepfakeDetector:\n",
      "\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n",
      "\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([2]).), using pretrained\n",
      "✓ Loading saved Wav2Vec2 weights\n",
      "  Warning: Could not load weights (Error(s) in loading state_dict for Wav2Vec2Detector:\n",
      "\tsize mismatch for classifier.0.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([256, 768]).\n",
      "\tsize mismatch for classifier.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([2, 256]).\n",
      "\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([2]).), using pretrained\n",
      "✓ Loading saved 3D ResNet weights\n",
      "  3D ResNet loaded successfully!\n",
      "============================================================\n",
      "\n",
      "Individual models loaded and frozen!\n",
      "Ready for late fusion training...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load image model\n",
    "image_model = ConvNeXtDeepfakeDetector(freeze_backbone=True)\n",
    "if os.path.exists('best_image_model_convnext.pth'):\n",
    "    print(\"✓ Loading saved ConvNeXt weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_image_model_convnext.pth', map_location=device)\n",
    "        image_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  ConvNeXt loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved ConvNeXt found, using pretrained from timm\")\n",
    "\n",
    "# Load audio model\n",
    "audio_model = Wav2Vec2Detector(freeze_backbone=True)\n",
    "if os.path.exists('best_audio_model_wav2vec2.pth'):\n",
    "    print(\"✓ Loading saved Wav2Vec2 weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_audio_model_wav2vec2.pth', map_location=device)\n",
    "        audio_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  Wav2Vec2 loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved Wav2Vec2 found, using pretrained from HuggingFace\")\n",
    "\n",
    "# Load video model\n",
    "video_model = ResNet3DDetector(pretrained=True)\n",
    "if os.path.exists('best_video_model_resnet3d.pth'):\n",
    "    print(\"✓ Loading saved 3D ResNet weights\")\n",
    "    try:\n",
    "        state_dict = torch.load('best_video_model_resnet3d.pth', map_location=device)\n",
    "        video_model.load_state_dict(state_dict, strict=False)\n",
    "        print(\"  3D ResNet loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not load weights ({e}), using pretrained\")\n",
    "else:\n",
    "    print(\"✗ No saved 3D ResNet found, using pretrained from torchvision\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Freeze all individual model parameters\n",
    "for param in image_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in audio_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in video_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\nIndividual models loaded and frozen!\")\n",
    "print(\"Ready for late fusion training...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
